{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ Train Unified Classifier - C·∫£i thi·ªán v·ªõi EMNIST, Focal Loss v√† RGB Shapes\n",
        "\n",
        "H·ªá th·ªëng nh·∫≠n di·ªán m·ªü r·ªông:\n",
        "- **10 ch·ªØ s·ªë**: 0-9 (MNIST + EMNIST digits)\n",
        "- **52 ch·ªØ c√°i**: A-Z, a-z (EMNIST letters)\n",
        "- **9 h√¨nh h·ªçc**: Circle, Triangle, Square, Pentagon, Hexagon, Heptagon, Octagon, Nonagon, Star\n",
        "\n",
        "## C·∫£i ti·∫øn m·ªõi:\n",
        "- ‚úÖ **MNIST + EMNIST**: TƒÉng d·ªØ li·ªáu digits v√† th√™m letters\n",
        "- ‚úÖ **Focal Loss + Class Weight**: Gi·∫£m l·ªói nh·∫ßm (4‚Üî9, 3‚Üî5, 6‚Üî5, 1‚Üî7)\n",
        "- ‚úÖ **RGB cho Shapes**: Nh·∫≠n di·ªán ƒë∆∞·ª£c background v√† fill m√†u kh√°c nhau\n",
        "- ‚úÖ **Augmentation m·∫°nh h∆°n**: Rotation 45¬∞, Perspective, ColorJitter m·∫°nh cho shapes\n",
        "- ‚úÖ **Top-2 Prediction Logic**: X·ª≠ l√Ω c√°c tr∆∞·ªùng h·ª£p nh·∫ßm trong top-2\n",
        "- ‚úÖ **INPUT_SIZE: 128x128** (ƒë·ªÉ ph√¢n bi·ªát t·ªët h∆°n)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ PyTorch version: 2.5.1+cu121\n",
            "‚úÖ CUDA available: True\n",
            "‚úÖ GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n"
          ]
        }
      ],
      "source": [
        "# Import Libraries\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
        "from torchvision.datasets import EMNIST\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
        "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CONFIGURATION\n",
            "============================================================\n",
            "Device: cuda\n",
            "Epochs: 25\n",
            "Batch size: 64\n",
            "Learning rate: 0.0001\n",
            "Input size: 128x128\n",
            "Num classes: 71\n",
            "  - Digits (0-9): 10\n",
            "  - Letters A-Z (10-35): 26\n",
            "  - Letters a-z (36-61): 26\n",
            "  - Shapes (62-70): 9\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "class Config:\n",
        "    # Paths\n",
        "    MNIST_TRAIN_DIR = 'mnist_competition/train'\n",
        "    MNIST_TRAIN_CSV = 'mnist_competition/train_label.csv'\n",
        "    SHAPES_DIR = 'Shapes_Classifier/dataset/output'\n",
        "    EMNIST_DATA_DIR = './data/emnist'\n",
        "    \n",
        "    # Training\n",
        "    EPOCHS = 25  # Increased from 20\n",
        "    BATCH_SIZE = 64\n",
        "    LEARNING_RATE = 1e-4\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    \n",
        "    # Model\n",
        "    # Classes: 0-9 (digits), 10-35 (A-Z), 36-61 (a-z), 62-70 (shapes)\n",
        "    NUM_CLASSES = 71  # 10 + 26 + 26 + 9\n",
        "    INPUT_SIZE = 128  # Increased from 64 to better distinguish high-edge shapes\n",
        "    \n",
        "    # Output\n",
        "    MODEL_PATH = 'unified_model_71classes_improved.pth'\n",
        "    LABEL_MAPPING_PATH = 'label_mapping_71classes.json'\n",
        "    \n",
        "    # Class weights cho c√°c s·ªë d·ªÖ nh·∫ßm (d·ª±a tr√™n ph√¢n t√≠ch l·ªói)\n",
        "    DIGIT_WEIGHTS = {\n",
        "        0: 1.0,   # 0\n",
        "        1: 2.0,   # 1 - d·ªÖ nh·∫ßm v·ªõi 7 (65.5%)\n",
        "        2: 1.5,   # 2 - d·ªÖ nh·∫ßm v·ªõi 3, 1\n",
        "        3: 2.0,   # 3 - d·ªÖ nh·∫ßm v·ªõi 5 (50%)\n",
        "        4: 2.5,   # 4 - d·ªÖ nh·∫ßm v·ªõi 9 (75%) - nghi√™m tr·ªçng nh·∫•t\n",
        "        5: 2.0,   # 5 - d·ªÖ nh·∫ßm v·ªõi 3 (67.7%)\n",
        "        6: 2.0,   # 6 - d·ªÖ nh·∫ßm v·ªõi 5 (62.7%)\n",
        "        7: 1.5,   # 7 - d·ªÖ nh·∫ßm v·ªõi 3 (41.7%)\n",
        "        8: 1.5,   # 8 - d·ªÖ nh·∫ßm nhi·ªÅu s·ªë\n",
        "        9: 2.5,   # 9 - d·ªÖ nh·∫ßm v·ªõi 4 (48.5%) - nghi√™m tr·ªçng nh·∫•t\n",
        "    }\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"CONFIGURATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Device: {Config.DEVICE}\")\n",
        "print(f\"Epochs: {Config.EPOCHS}\")\n",
        "print(f\"Batch size: {Config.BATCH_SIZE}\")\n",
        "print(f\"Learning rate: {Config.LEARNING_RATE}\")\n",
        "print(f\"Input size: {Config.INPUT_SIZE}x{Config.INPUT_SIZE}\")\n",
        "print(f\"Num classes: {Config.NUM_CLASSES}\")\n",
        "print(f\"  - Digits (0-9): 10\")\n",
        "print(f\"  - Letters A-Z (10-35): 26\")\n",
        "print(f\"  - Letters a-z (36-61): 26\")\n",
        "print(f\"  - Shapes (62-70): 9\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading datasets...\n",
            "‚úÖ MNIST: 60000 images\n",
            "\n",
            "üìÇ Loading EMNIST...\n",
            "Downloading https://biometrics.nist.gov/cs_links/EMNIST/gzip.zip to ./data/emnist\\EMNIST\\raw\\gzip.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 562M/562M [00:50<00:00, 11.2MB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/emnist\\EMNIST\\raw\\gzip.zip to ./data/emnist\\EMNIST\\raw\n",
            "‚úÖ EMNIST train: 112800 images\n",
            "‚úÖ EMNIST test: 18800 images\n",
            "\n",
            "üìù Converting EMNIST to image files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 112800/112800 [01:26<00:00, 1305.16it/s]\n",
            "Processing test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18800/18800 [00:14<00:00, 1277.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Combined digits: 88000 images (MNIST + EMNIST)\n",
            "‚úÖ EMNIST letters: 103600 images\n",
            "‚úÖ Shapes: 90000 images\n",
            "\n",
            "üìã Label Mapping (71 classes):\n",
            "   Class  0: 0\n",
            "   Class  1: 1\n",
            "   Class 10: A\n",
            "   Class 11: B\n",
            "   Class 12: C\n",
            "   Class 13: D\n",
            "   Class 14: E\n",
            "   Class 15: F\n",
            "   Class 16: G\n",
            "   Class 17: H\n",
            "   Class 18: I\n",
            "   Class 19: J\n",
            "   Class  2: 2\n",
            "   Class 20: K\n",
            "   Class 21: L\n",
            "   Class 22: M\n",
            "   Class 23: N\n",
            "   Class 24: O\n",
            "   Class 25: P\n",
            "   Class 26: Q\n",
            "   Class 27: R\n",
            "   Class 28: S\n",
            "   Class 29: T\n",
            "   Class  3: 3\n",
            "   Class 30: U\n",
            "   Class 31: V\n",
            "   Class 32: W\n",
            "   Class 33: X\n",
            "   Class 34: Y\n",
            "   Class 35: Z\n",
            "   Class 36: a\n",
            "   Class 37: b\n",
            "   Class 38: c\n",
            "   Class 39: d\n",
            "   Class  4: 4\n",
            "   Class 40: e\n",
            "   Class 41: f\n",
            "   Class 42: g\n",
            "   Class 43: h\n",
            "   Class 44: i\n",
            "   Class 45: j\n",
            "   Class 46: k\n",
            "   Class 47: l\n",
            "   Class 48: m\n",
            "   Class 49: n\n",
            "   Class  5: 5\n",
            "   Class 50: o\n",
            "   Class 51: p\n",
            "   Class 52: q\n",
            "   Class 53: r\n",
            "   Class 54: s\n",
            "   Class 55: t\n",
            "   Class 56: u\n",
            "   Class 57: v\n",
            "   Class 58: w\n",
            "   Class 59: x\n",
            "   Class  6: 6\n",
            "   Class 60: y\n",
            "   Class 61: z\n",
            "   Class 62: Circle\n",
            "   Class 63: Heptagon\n",
            "   Class 64: Hexagon\n",
            "   Class 65: Nonagon\n",
            "   Class 66: Octagon\n",
            "   Class 67: Pentagon\n",
            "   Class 68: Square\n",
            "   Class 69: Star\n",
            "   Class  7: 7\n",
            "   Class 70: Triangle\n",
            "   Class  8: 8\n",
            "   Class  9: 9\n",
            "\n",
            "‚úÖ Saved label_mapping_71classes.json\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading datasets...\")\n",
        "\n",
        "# Load MNIST\n",
        "mnist_df = pd.read_csv(Config.MNIST_TRAIN_CSV)\n",
        "mnist_df['source'] = 'mnist'  # Th√™m c·ªôt source ƒë·ªÉ ph√¢n bi·ªát v·ªõi EMNIST\n",
        "print(f\"‚úÖ MNIST: {len(mnist_df)} images\")\n",
        "\n",
        "# Load EMNIST\n",
        "print(\"\\nüìÇ Loading EMNIST...\")\n",
        "emnist_data = []\n",
        "emnist_letters_data = []\n",
        "os.makedirs('emnist_images', exist_ok=True)\n",
        "\n",
        "try:\n",
        "    # EMNIST balanced: 47 classes (0-9 digits, 10-35 A-Z, 36-61 a-z)\n",
        "    emnist_train = EMNIST(\n",
        "        root=Config.EMNIST_DATA_DIR,\n",
        "        split='balanced',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=None\n",
        "    )\n",
        "    emnist_test = EMNIST(\n",
        "        root=Config.EMNIST_DATA_DIR,\n",
        "        split='balanced',\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=None\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úÖ EMNIST train: {len(emnist_train)} images\")\n",
        "    print(f\"‚úÖ EMNIST test: {len(emnist_test)} images\")\n",
        "    \n",
        "    print(\"\\nüìù Converting EMNIST to image files...\")\n",
        "    for dataset, split_name in [(emnist_train, 'train'), (emnist_test, 'test')]:\n",
        "        for idx, (img, label) in enumerate(tqdm(dataset, desc=f\"Processing {split_name}\")):\n",
        "            img_path = f'emnist_images/emnist_{split_name}_{idx:06d}.png'\n",
        "            img.save(img_path)\n",
        "            \n",
        "            label_int = int(label)\n",
        "            if label_int < 10:\n",
        "                # Digits: merge v·ªõi MNIST\n",
        "                emnist_data.append({\n",
        "                    'image_name': os.path.basename(img_path),\n",
        "                    'label': label_int,\n",
        "                    'source': 'emnist'\n",
        "                })\n",
        "            else:\n",
        "                # Letters: gi·ªØ nguy√™n label (10-61)\n",
        "                emnist_letters_data.append({\n",
        "                    'image_name': os.path.basename(img_path),\n",
        "                    'label': label_int,\n",
        "                    'source': 'emnist'\n",
        "                })\n",
        "    \n",
        "    emnist_digits_df = pd.DataFrame(emnist_data)\n",
        "    emnist_letters_df = pd.DataFrame(emnist_letters_data)\n",
        "    \n",
        "    # Merge MNIST + EMNIST digits\n",
        "    combined_digits_df = pd.concat([mnist_df, emnist_digits_df], ignore_index=True)\n",
        "    print(f\"\\n‚úÖ Combined digits: {len(combined_digits_df)} images (MNIST + EMNIST)\")\n",
        "    print(f\"‚úÖ EMNIST letters: {len(emnist_letters_df)} images\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading EMNIST: {e}\")\n",
        "    print(\"‚ö†Ô∏è  S·ª≠ d·ª•ng ch·ªâ MNIST (kh√¥ng c√≥ EMNIST)\")\n",
        "    combined_digits_df = mnist_df.copy()\n",
        "    emnist_letters_df = pd.DataFrame(columns=['image_name', 'label', 'source'])\n",
        "\n",
        "# Load Shapes\n",
        "shape_files = [f for f in os.listdir(Config.SHAPES_DIR) if f.endswith('.png')]\n",
        "shape_labels = [f.split('_')[0] for f in shape_files]\n",
        "shapes_df = pd.DataFrame({'image_name': shape_files, 'label': shape_labels})\n",
        "print(f\"‚úÖ Shapes: {len(shapes_df)} images\")\n",
        "\n",
        "# Create label mapping\n",
        "shape_names = sorted(shapes_df['label'].unique())\n",
        "shape_to_id = {name: idx + 62 for idx, name in enumerate(shape_names)}  # 62-70\n",
        "\n",
        "# T·∫°o id_to_label mapping ƒë·∫ßy ƒë·ªß\n",
        "id_to_label = {}\n",
        "# Digits 0-9\n",
        "for i in range(10):\n",
        "    id_to_label[str(i)] = str(i)\n",
        "# Letters A-Z (10-35)\n",
        "for i, letter in enumerate('ABCDEFGHIJKLMNOPQRSTUVWXYZ'):\n",
        "    id_to_label[str(i + 10)] = letter\n",
        "# Letters a-z (36-61)\n",
        "for i, letter in enumerate('abcdefghijklmnopqrstuvwxyz'):\n",
        "    id_to_label[str(i + 36)] = letter\n",
        "# Shapes (62-70)\n",
        "for name, class_id in shape_to_id.items():\n",
        "    id_to_label[str(class_id)] = name\n",
        "\n",
        "print(f\"\\nüìã Label Mapping ({len(id_to_label)} classes):\")\n",
        "for class_id, label_name in sorted(id_to_label.items()):\n",
        "    print(f\"   Class {int(class_id):2d}: {label_name}\")\n",
        "\n",
        "# Save label mapping\n",
        "with open(Config.LABEL_MAPPING_PATH, 'w') as f:\n",
        "    json.dump(id_to_label, f, indent=2)\n",
        "print(f\"\\n‚úÖ Saved {Config.LABEL_MAPPING_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ UnifiedDataset class defined\n"
          ]
        }
      ],
      "source": [
        "class UnifiedDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Unified dataset for digits, letters and geometric shapes.\n",
        "    - Digits/Letters: Grayscale (convert to RGB 3 channels)\n",
        "    - Shapes: RGB (keep color to recognize background/fill)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, digits_df, letters_df, shapes_df, \n",
        "                 mnist_dir, emnist_dir, shapes_dir,\n",
        "                 shape_to_id, transform=None, sample_fraction=0.67):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            digits_df: DataFrame with digits data (MNIST + EMNIST)\n",
        "            letters_df: DataFrame with letters data (EMNIST)\n",
        "            shapes_df: DataFrame with shapes data\n",
        "            mnist_dir: Directory with MNIST images\n",
        "            emnist_dir: Directory with EMNIST images\n",
        "            shapes_dir: Directory with shape images\n",
        "            shape_to_id: Mapping from shape name to class ID (62-70)\n",
        "            transform: Image transforms\n",
        "            sample_fraction: Fraction of shapes to use (balance with digits)\n",
        "        \"\"\"\n",
        "        self.data_list = []\n",
        "        \n",
        "        # Add digits (0-9)\n",
        "        if digits_df is not None and len(digits_df) > 0:\n",
        "            for idx, row in digits_df.iterrows():\n",
        "                # Ki·ªÉm tra source ƒë·ªÉ x√°c ƒë·ªãnh th∆∞ m·ª•c ƒë√∫ng\n",
        "                source = row.get('source', 'mnist')  # M·∫∑c ƒë·ªãnh l√† 'mnist' n·∫øu kh√¥ng c√≥\n",
        "                if source == 'mnist':\n",
        "                    img_path = os.path.join(mnist_dir, row['image_name'])\n",
        "                else:  # emnist\n",
        "                    img_path = os.path.join(emnist_dir, row['image_name'])\n",
        "                \n",
        "                # Ki·ªÉm tra file t·ªìn t·∫°i tr∆∞·ªõc khi th√™m\n",
        "                if os.path.exists(img_path):\n",
        "                    self.data_list.append({\n",
        "                        'path': img_path,\n",
        "                        'label': int(row['label']),  # 0-9\n",
        "                        'source': 'digit'\n",
        "                    })\n",
        "                else:\n",
        "                    print(f\"‚ö†Ô∏è  Warning: File not found: {img_path}\")\n",
        "        \n",
        "        # Add letters (10-61)\n",
        "        if letters_df is not None and len(letters_df) > 0:\n",
        "            for idx, row in letters_df.iterrows():\n",
        "                img_path = os.path.join(emnist_dir, row['image_name'])\n",
        "                # Ki·ªÉm tra file t·ªìn t·∫°i tr∆∞·ªõc khi th√™m\n",
        "                if os.path.exists(img_path):\n",
        "                    self.data_list.append({\n",
        "                        'path': img_path,\n",
        "                        'label': int(row['label']),  # 10-61\n",
        "                        'source': 'letter'\n",
        "                    })\n",
        "                else:\n",
        "                    print(f\"‚ö†Ô∏è  Warning: File not found: {img_path}\")\n",
        "        \n",
        "        # Add shapes (62-70)\n",
        "        if shapes_df is not None:\n",
        "            shapes_df_sampled = shapes_df.sample(frac=sample_fraction, random_state=42)\n",
        "            for idx, row in shapes_df_sampled.iterrows():\n",
        "                img_path = os.path.join(shapes_dir, row['image_name'])\n",
        "                # Ki·ªÉm tra file t·ªìn t·∫°i tr∆∞·ªõc khi th√™m\n",
        "                if os.path.exists(img_path):\n",
        "                    self.data_list.append({\n",
        "                        'path': img_path,\n",
        "                        'label': shape_to_id[row['label']],  # 62-70\n",
        "                        'source': 'shape'\n",
        "                    })\n",
        "                else:\n",
        "                    print(f\"‚ö†Ô∏è  Warning: File not found: {img_path}\")\n",
        "        \n",
        "        self.transform = transform\n",
        "        \n",
        "        # Statistics\n",
        "        digit_count = sum(1 for item in self.data_list if item['source'] == 'digit')\n",
        "        letter_count = sum(1 for item in self.data_list if item['source'] == 'letter')\n",
        "        shape_count = sum(1 for item in self.data_list if item['source'] == 'shape')\n",
        "        \n",
        "        print(f\"‚úÖ Dataset created: {len(self.data_list)} images\")\n",
        "        print(f\"   - Digits: {digit_count} images (classes 0-9)\")\n",
        "        print(f\"   - Letters: {letter_count} images (classes 10-61)\")\n",
        "        print(f\"   - Shapes: {shape_count} images (classes 62-70)\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data_list[idx]\n",
        "        \n",
        "        # Shapes: Keep RGB to recognize background/fill colors\n",
        "        # Digits/Letters: Grayscale (will convert to RGB in transform)\n",
        "        if item['source'] == 'shape':\n",
        "            image = Image.open(item['path']).convert('RGB')  # Keep RGB for shapes\n",
        "        else:\n",
        "            image = Image.open(item['path']).convert('L')  # Grayscale for digits/letters\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image, is_shape=(item['source'] == 'shape'))\n",
        "        \n",
        "        return image, item['label']\n",
        "\n",
        "print(\"‚úÖ UnifiedDataset class defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèãÔ∏è Training Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Training functions defined\n"
          ]
        }
      ],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Focal Loss ƒë·ªÉ t·∫≠p trung v√†o c√°c m·∫´u kh√≥ ph√¢n bi·ªát.\n",
        "    Gi√∫p gi·∫£m l·ªói nh·∫ßm gi·ªØa c√°c s·ªë t∆∞∆°ng t·ª± (4‚Üî9, 3‚Üî5, 6‚Üî5, 1‚Üî7).\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha=None, gamma=2.0, weight=None):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.weight = weight\n",
        "        \n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, weight=self.weight, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
        "        \n",
        "        if self.alpha is not None:\n",
        "            if isinstance(self.alpha, (list, np.ndarray)):\n",
        "                alpha_t = torch.tensor(self.alpha, device=inputs.device)\n",
        "                alpha_t = alpha_t[targets]\n",
        "                focal_loss = alpha_t * focal_loss\n",
        "        \n",
        "        return focal_loss.mean()\n",
        "\n",
        "def create_class_weights():\n",
        "    \"\"\"T·∫°o class weights d·ª±a tr√™n ph√¢n t√≠ch l·ªói.\"\"\"\n",
        "    weights = torch.ones(Config.NUM_CLASSES)\n",
        "    \n",
        "    # Digits (0-9): √Åp d·ª•ng weights t·ª´ Config\n",
        "    for digit, weight in Config.DIGIT_WEIGHTS.items():\n",
        "        weights[digit] = weight\n",
        "    \n",
        "    return weights.to(Config.DEVICE)\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    pbar = tqdm(loader, desc='Training')\n",
        "    for images, labels in pbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        \n",
        "        pbar.set_postfix({\n",
        "            'loss': f\"{running_loss/(pbar.n+1):.4f}\",\n",
        "            'acc': f\"{100.*correct/total:.2f}%\"\n",
        "        })\n",
        "    \n",
        "    return running_loss / len(loader), 100. * correct / total\n",
        "\n",
        "def validate(model, loader, criterion, device, use_top2=False):\n",
        "    \"\"\"Validate model v·ªõi option s·ª≠ d·ª•ng top-2 logic.\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(loader, desc='Validation')\n",
        "        for images, labels in pbar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            if use_top2:\n",
        "                # S·ª≠ d·ª•ng top-2 logic\n",
        "                batch_preds = []\n",
        "                for i in range(images.size(0)):\n",
        "                    pred, _, _ = predict_with_top2(model, images[i], device)\n",
        "                    batch_preds.append(pred.item())\n",
        "                predicted = torch.tensor(batch_preds, device=device)\n",
        "            else:\n",
        "                _, predicted = outputs.max(1)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            \n",
        "            pbar.set_postfix({\n",
        "                'loss': f\"{running_loss/(pbar.n+1):.4f}\",\n",
        "                'acc': f\"{100.*correct/total:.2f}%\"\n",
        "            })\n",
        "    \n",
        "    return running_loss / len(loader), 100. * correct / total\n",
        "\n",
        "print(\"‚úÖ Training functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ B∆∞·ªõc 4: Augmentation m·∫°nh h∆°n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Data Split:\n",
            "   Train: Digits 74800 + Letters 88060 + Shapes ~51255\n",
            "   Val:   Digits 13200 + Letters 15540 + Shapes ~9045\n"
          ]
        }
      ],
      "source": [
        "class AdaptiveTransform:\n",
        "    \"\"\"\n",
        "    Transform th√≠ch ·ª©ng: \n",
        "    - Digits/Letters: Grayscale -> RGB (3 channels gi·ªëng nhau)\n",
        "    - Shapes: RGB (gi·ªØ nguy√™n m√†u, augmentation m√†u m·∫°nh ƒë·ªÉ h·ªçc background/fill)\n",
        "    \"\"\"\n",
        "    def __init__(self, is_train=True):\n",
        "        self.is_train = is_train\n",
        "        \n",
        "    def __call__(self, image, is_shape=False):\n",
        "        # Resize tr∆∞·ªõc\n",
        "        image = transforms.Resize((Config.INPUT_SIZE, Config.INPUT_SIZE))(image)\n",
        "        \n",
        "        if is_shape:\n",
        "            # Shapes: RGB v·ªõi augmentation m√†u m·∫°nh ƒë·ªÉ h·ªçc c√°c m√†u kh√°c nhau\n",
        "            if self.is_train:\n",
        "                image = transforms.RandomRotation(45)(image)\n",
        "                image = transforms.RandomAffine(\n",
        "                    degrees=0,\n",
        "                    translate=(0.2, 0.2),\n",
        "                    scale=(0.8, 1.2),\n",
        "                    shear=10\n",
        "                )(image)\n",
        "                image = transforms.RandomPerspective(distortion_scale=0.3, p=0.5)(image)\n",
        "                # Color augmentation M·∫†NH cho shapes ƒë·ªÉ h·ªçc m√†u background/fill\n",
        "                image = transforms.ColorJitter(\n",
        "                    brightness=0.5,    # TƒÉng ƒë·ªÉ h·ªçc c√°c m√†u background kh√°c nhau\n",
        "                    contrast=0.5,      # TƒÉng ƒë·ªÉ h·ªçc c√°c m√†u fill kh√°c nhau\n",
        "                    saturation=0.5,    # Th√™m saturation cho shapes\n",
        "                    hue=0.1            # Th√™m hue variation\n",
        "                )(image)\n",
        "                image = transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 0.5))(image)\n",
        "        else:\n",
        "            # Digits/Letters: Grayscale -> RGB\n",
        "            image = transforms.Grayscale(num_output_channels=3)(image)\n",
        "            \n",
        "            if self.is_train:\n",
        "                image = transforms.RandomRotation(45)(image)\n",
        "                image = transforms.RandomAffine(\n",
        "                    degrees=0,\n",
        "                    translate=(0.2, 0.2),\n",
        "                    scale=(0.8, 1.2),\n",
        "                    shear=10\n",
        "                )(image)\n",
        "                image = transforms.RandomPerspective(distortion_scale=0.3, p=0.5)(image)\n",
        "                # Color jitter nh·∫π h∆°n cho grayscale\n",
        "                image = transforms.ColorJitter(brightness=0.4, contrast=0.4)(image)\n",
        "                image = transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 0.5))(image)\n",
        "        \n",
        "        # ToTensor v√† Normalize\n",
        "        image = transforms.ToTensor()(image)\n",
        "        image = transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )(image)\n",
        "        \n",
        "        return image\n",
        "\n",
        "# T·∫°o transforms\n",
        "train_transform = AdaptiveTransform(is_train=True)\n",
        "val_transform = AdaptiveTransform(is_train=False)\n",
        "\n",
        "# Split data\n",
        "digits_train, digits_val = train_test_split(\n",
        "    combined_digits_df, test_size=0.15, random_state=42, \n",
        "    stratify=combined_digits_df['label'] if 'label' in combined_digits_df.columns else None\n",
        ")\n",
        "\n",
        "if len(emnist_letters_df) > 0:\n",
        "    letters_train, letters_val = train_test_split(\n",
        "        emnist_letters_df, test_size=0.15, random_state=42,\n",
        "        stratify=emnist_letters_df['label'] if 'label' in emnist_letters_df.columns else None\n",
        "    )\n",
        "else:\n",
        "    letters_train = pd.DataFrame(columns=['image_name', 'label', 'source'])\n",
        "    letters_val = pd.DataFrame(columns=['image_name', 'label', 'source'])\n",
        "\n",
        "shapes_train, shapes_val = train_test_split(\n",
        "    shapes_df, test_size=0.15, random_state=42, stratify=shapes_df['label']\n",
        ")\n",
        "\n",
        "print(f\"üìä Data Split:\")\n",
        "print(f\"   Train: Digits {len(digits_train)} + Letters {len(letters_train)} + Shapes ~{int(len(shapes_train)*0.67)}\")\n",
        "print(f\"   Val:   Digits {len(digits_val)} + Letters {len(letters_val)} + Shapes ~{int(len(shapes_val)*0.67)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Top-2 Prediction Logic (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Top-2 prediction logic defined\n"
          ]
        }
      ],
      "source": [
        "def apply_top2_logic(predicted, top2_indices, top2_probs, threshold=0.1):\n",
        "    \"\"\"X·ª≠ l√Ω c√°c tr∆∞·ªùng h·ª£p nh·∫ßm trong top-2 predictions.\"\"\"\n",
        "    prob_diff = top2_probs[0] - top2_probs[1]\n",
        "    \n",
        "    if prob_diff < threshold:\n",
        "        pred_class = top2_indices[0].item()\n",
        "        second_class = top2_indices[1].item()\n",
        "        \n",
        "        confusion_pairs = {\n",
        "            (4, 9): 'mirror', (9, 4): 'mirror',\n",
        "            (3, 5): 'top_curve', (5, 3): 'top_curve',\n",
        "            (6, 5): 'bottom_curve', (5, 6): 'bottom_curve',\n",
        "            (1, 7): 'diagonal', (7, 1): 'diagonal',\n",
        "            (0, 8): 'thickness', (8, 0): 'thickness',\n",
        "        }\n",
        "        \n",
        "        if (pred_class, second_class) in confusion_pairs:\n",
        "            return predicted\n",
        "    \n",
        "    return predicted\n",
        "\n",
        "def predict_with_top2(model, image, device, threshold=0.1):\n",
        "    \"\"\"Predict v·ªõi x·ª≠ l√Ω top-2 logic.\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        image = image.unsqueeze(0).to(device)\n",
        "        outputs = model(image)\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "        top2_probs, top2_indices = probs.topk(2, dim=1)\n",
        "        predicted = apply_top2_logic(\n",
        "            top2_indices[0, 0], top2_indices[0], top2_probs[0], threshold\n",
        "        )\n",
        "        return predicted, top2_probs[0], top2_indices[0]\n",
        "\n",
        "print(\"‚úÖ Top-2 prediction logic defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé≤ Create Datasets & DataLoaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Dataset created: 214115 images\n",
            "   - Digits: 74800 images (classes 0-9)\n",
            "   - Letters: 88060 images (classes 10-61)\n",
            "   - Shapes: 51255 images (classes 62-70)\n",
            "‚úÖ Dataset created: 37785 images\n",
            "   - Digits: 13200 images (classes 0-9)\n",
            "   - Letters: 15540 images (classes 10-61)\n",
            "   - Shapes: 9045 images (classes 62-70)\n",
            "\n",
            "‚úÖ DataLoaders ready\n",
            "   Train batches: 3346\n",
            "   Val batches: 591\n"
          ]
        }
      ],
      "source": [
        "# Create datasets\n",
        "train_dataset = UnifiedDataset(\n",
        "    digits_train, letters_train, shapes_train,\n",
        "    Config.MNIST_TRAIN_DIR, 'emnist_images', Config.SHAPES_DIR,\n",
        "    shape_to_id, transform=train_transform,\n",
        "    sample_fraction=0.67\n",
        ")\n",
        "\n",
        "val_dataset = UnifiedDataset(\n",
        "    digits_val, letters_val, shapes_val,\n",
        "    Config.MNIST_TRAIN_DIR, 'emnist_images', Config.SHAPES_DIR,\n",
        "    shape_to_id, transform=val_transform,\n",
        "    sample_fraction=0.67\n",
        ")\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=Config.BATCH_SIZE,\n",
        "    shuffle=True, num_workers=0, pin_memory=True\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, batch_size=Config.BATCH_SIZE,\n",
        "    shuffle=False, num_workers=0, pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ DataLoaders ready\")\n",
        "print(f\"   Train batches: {len(train_loader)}\")\n",
        "print(f\"   Val batches: {len(val_loader)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Load Model: EfficientNet-B0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading EfficientNet-B0...\n",
            "‚úÖ Model ready\n",
            "   Total parameters: 4,098,499\n",
            "   Trainable parameters: 4,098,499\n",
            "   Model on device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading EfficientNet-B0...\")\n",
        "model = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Modify classifier for 71 classes\n",
        "num_features = model.classifier[1].in_features\n",
        "model.classifier[1] = nn.Linear(num_features, Config.NUM_CLASSES)\n",
        "model = model.to(Config.DEVICE)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"‚úÖ Model ready\")\n",
        "print(f\"   Total parameters: {total_params:,}\")\n",
        "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"   Model on device: {next(model.parameters()).device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Training Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Training setup complete\n",
            "   Criterion: FocalLoss (gamma=2.0) v·ªõi class weights\n",
            "   Optimizer: Adam (lr=0.0001)\n",
            "   Scheduler: ReduceLROnPlateau (factor=0.5, patience=3)\n",
            "\n",
            "Class weights (digits):\n",
            "   Class 0: 1.00x\n",
            "   Class 1: 2.00x\n",
            "   Class 2: 1.50x\n",
            "   Class 3: 2.00x\n",
            "   Class 4: 2.50x\n",
            "   Class 5: 2.00x\n",
            "   Class 6: 2.00x\n",
            "   Class 7: 1.50x\n",
            "   Class 8: 1.50x\n",
            "   Class 9: 2.50x\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Vu The Van\\anaconda3\\envs\\httt\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# T·∫°o class weights\n",
        "class_weights = create_class_weights()\n",
        "\n",
        "# Focal Loss v·ªõi class weights\n",
        "criterion = FocalLoss(alpha=None, gamma=2.0, weight=class_weights)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)\n",
        "\n",
        "# Scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='max', factor=0.5, patience=3, verbose=True\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Training setup complete\")\n",
        "print(f\"   Criterion: FocalLoss (gamma=2.0) v·ªõi class weights\")\n",
        "print(f\"   Optimizer: Adam (lr={Config.LEARNING_RATE})\")\n",
        "print(f\"   Scheduler: ReduceLROnPlateau (factor=0.5, patience=3)\")\n",
        "print(f\"\\nClass weights (digits):\")\n",
        "for i in range(10):\n",
        "    print(f\"   Class {i}: {class_weights[i]:.2f}x\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Training Loop\n",
        "\n",
        "**‚ö†Ô∏è Warning:** This will take ~1-2 hours depending on your GPU!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "STARTING TRAINING\n",
            "============================================================\n",
            "\n",
            "\n",
            "Epoch 1/25\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3346/3346 [1:12:17<00:00,  1.30s/it, loss=0.7308, acc=73.02%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [08:57<00:00,  1.10it/s, loss=0.2149, acc=87.37%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [12:15<00:00,  1.24s/it, loss=0.2149, acc=87.38%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.7308 | Train Acc: 73.02%\n",
            "Val Loss: 0.2149 | Val Acc: 87.37%\n",
            "Val Acc (top-2): 87.38% (c·∫£i thi·ªán: +0.00%)\n",
            "‚úÖ Saved best model: unified_model_71classes_improved.pth\n",
            "   Val Acc: 87.37% | Val Acc (top-2): 87.38%\n",
            "\n",
            "Epoch 2/25\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3346/3346 [1:11:15<00:00,  1.28s/it, loss=0.2721, acc=85.24%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [07:28<00:00,  1.32it/s, loss=0.1402, acc=90.56%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [12:11<00:00,  1.24s/it, loss=0.1402, acc=90.56%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.2721 | Train Acc: 85.24%\n",
            "Val Loss: 0.1402 | Val Acc: 90.56%\n",
            "Val Acc (top-2): 90.56% (c·∫£i thi·ªán: +0.00%)\n",
            "‚úÖ Saved best model: unified_model_71classes_improved.pth\n",
            "   Val Acc: 90.56% | Val Acc (top-2): 90.56%\n",
            "\n",
            "Epoch 3/25\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3346/3346 [1:05:28<00:00,  1.17s/it, loss=0.2188, acc=87.43%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [07:41<00:00,  1.28it/s, loss=0.1251, acc=91.46%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [11:51<00:00,  1.20s/it, loss=0.1251, acc=91.45%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.2188 | Train Acc: 87.43%\n",
            "Val Loss: 0.1251 | Val Acc: 91.46%\n",
            "Val Acc (top-2): 91.45% (c·∫£i thi·ªán: +-0.00%)\n",
            "‚úÖ Saved best model: unified_model_71classes_improved.pth\n",
            "   Val Acc: 91.46% | Val Acc (top-2): 91.45%\n",
            "\n",
            "Epoch 4/25\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3346/3346 [58:53<00:00,  1.06s/it, loss=0.1912, acc=88.57%] \n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [02:23<00:00,  4.11it/s, loss=0.1124, acc=92.17%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [11:34<00:00,  1.18s/it, loss=0.1124, acc=92.17%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1912 | Train Acc: 88.57%\n",
            "Val Loss: 0.1124 | Val Acc: 92.17%\n",
            "Val Acc (top-2): 92.17% (c·∫£i thi·ªán: +0.00%)\n",
            "‚úÖ Saved best model: unified_model_71classes_improved.pth\n",
            "   Val Acc: 92.17% | Val Acc (top-2): 92.17%\n",
            "\n",
            "Epoch 5/25\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3346/3346 [36:39<00:00,  1.52it/s, loss=0.1739, acc=89.33%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [02:19<00:00,  4.23it/s, loss=0.1105, acc=92.38%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [11:34<00:00,  1.17s/it, loss=0.1105, acc=92.38%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1739 | Train Acc: 89.33%\n",
            "Val Loss: 0.1105 | Val Acc: 92.38%\n",
            "Val Acc (top-2): 92.38% (c·∫£i thi·ªán: +-0.00%)\n",
            "‚úÖ Saved best model: unified_model_71classes_improved.pth\n",
            "   Val Acc: 92.38% | Val Acc (top-2): 92.38%\n",
            "\n",
            "Epoch 6/25\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3346/3346 [37:25<00:00,  1.49it/s, loss=0.1592, acc=90.01%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [02:17<00:00,  4.30it/s, loss=0.1010, acc=92.90%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [11:30<00:00,  1.17s/it, loss=0.1010, acc=92.90%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1592 | Train Acc: 90.01%\n",
            "Val Loss: 0.1010 | Val Acc: 92.90%\n",
            "Val Acc (top-2): 92.90% (c·∫£i thi·ªán: +0.00%)\n",
            "‚úÖ Saved best model: unified_model_71classes_improved.pth\n",
            "   Val Acc: 92.90% | Val Acc (top-2): 92.90%\n",
            "\n",
            "Epoch 7/25\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3346/3346 [52:13<00:00,  1.07it/s, loss=0.1488, acc=90.41%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [07:57<00:00,  1.24it/s, loss=0.0967, acc=93.24%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [11:34<00:00,  1.17s/it, loss=0.0967, acc=93.24%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1488 | Train Acc: 90.41%\n",
            "Val Loss: 0.0967 | Val Acc: 93.24%\n",
            "Val Acc (top-2): 93.24% (c·∫£i thi·ªán: +-0.00%)\n",
            "‚úÖ Saved best model: unified_model_71classes_improved.pth\n",
            "   Val Acc: 93.24% | Val Acc (top-2): 93.24%\n",
            "\n",
            "Epoch 8/25\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3346/3346 [1:10:53<00:00,  1.27s/it, loss=0.1440, acc=90.62%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [09:34<00:00,  1.03it/s, loss=0.0956, acc=93.00%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [13:25<00:00,  1.36s/it, loss=0.0956, acc=93.00%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1440 | Train Acc: 90.62%\n",
            "Val Loss: 0.0956 | Val Acc: 93.00%\n",
            "Val Acc (top-2): 93.00% (c·∫£i thi·ªán: +0.00%)\n",
            "\n",
            "Epoch 9/25\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3346/3346 [1:07:56<00:00,  1.22s/it, loss=0.1373, acc=91.01%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [08:51<00:00,  1.11it/s, loss=0.0940, acc=93.44%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [11:34<00:00,  1.18s/it, loss=0.0940, acc=93.44%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1373 | Train Acc: 91.01%\n",
            "Val Loss: 0.0940 | Val Acc: 93.44%\n",
            "Val Acc (top-2): 93.44% (c·∫£i thi·ªán: +-0.00%)\n",
            "‚úÖ Saved best model: unified_model_71classes_improved.pth\n",
            "   Val Acc: 93.44% | Val Acc (top-2): 93.44%\n",
            "\n",
            "Epoch 10/25\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3346/3346 [1:07:19<00:00,  1.21s/it, loss=0.1303, acc=91.26%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [07:19<00:00,  1.34it/s, loss=0.0909, acc=93.64%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [05:24<00:00,  1.82it/s, loss=0.0909, acc=93.64%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1303 | Train Acc: 91.26%\n",
            "Val Loss: 0.0909 | Val Acc: 93.64%\n",
            "Val Acc (top-2): 93.64% (c·∫£i thi·ªán: +-0.00%)\n",
            "‚úÖ Saved best model: unified_model_71classes_improved.pth\n",
            "   Val Acc: 93.64% | Val Acc (top-2): 93.64%\n",
            "\n",
            "Epoch 11/25\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3346/3346 [1:01:54<00:00,  1.11s/it, loss=0.1268, acc=91.45%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [11:09<00:00,  1.13s/it, loss=0.0920, acc=93.61%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [20:56<00:00,  2.13s/it, loss=0.0920, acc=93.60%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1268 | Train Acc: 91.45%\n",
            "Val Loss: 0.0920 | Val Acc: 93.61%\n",
            "Val Acc (top-2): 93.60% (c·∫£i thi·ªán: +-0.01%)\n",
            "\n",
            "Epoch 12/25\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3346/3346 [1:53:36<00:00,  2.04s/it, loss=0.1232, acc=91.63%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [04:06<00:00,  2.40it/s, loss=0.0897, acc=93.63%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [20:17<00:00,  2.06s/it, loss=0.0897, acc=93.62%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1232 | Train Acc: 91.63%\n",
            "Val Loss: 0.0897 | Val Acc: 93.63%\n",
            "Val Acc (top-2): 93.62% (c·∫£i thi·ªán: +-0.01%)\n",
            "\n",
            "Epoch 13/25\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3346/3346 [1:00:48<00:00,  1.09s/it, loss=0.1193, acc=91.83%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [04:04<00:00,  2.42it/s, loss=0.0867, acc=93.90%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [22:05<00:00,  2.24s/it, loss=0.0867, acc=93.90%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1193 | Train Acc: 91.83%\n",
            "Val Loss: 0.0867 | Val Acc: 93.90%\n",
            "Val Acc (top-2): 93.90% (c·∫£i thi·ªán: +0.00%)\n",
            "‚úÖ Saved best model: unified_model_71classes_improved.pth\n",
            "   Val Acc: 93.90% | Val Acc (top-2): 93.90%\n",
            "\n",
            "Epoch 14/25\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3346/3346 [55:29<00:00,  1.01it/s, loss=0.1167, acc=91.93%] \n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [07:23<00:00,  1.33it/s, loss=0.0881, acc=93.69%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [11:44<00:00,  1.19s/it, loss=0.0881, acc=93.69%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1167 | Train Acc: 91.93%\n",
            "Val Loss: 0.0881 | Val Acc: 93.69%\n",
            "Val Acc (top-2): 93.69% (c·∫£i thi·ªán: +-0.00%)\n",
            "\n",
            "Epoch 15/25\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3346/3346 [1:07:41<00:00,  1.21s/it, loss=0.1136, acc=92.05%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [07:51<00:00,  1.25it/s, loss=0.0894, acc=93.56%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [11:40<00:00,  1.18s/it, loss=0.0894, acc=93.56%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1136 | Train Acc: 92.05%\n",
            "Val Loss: 0.0894 | Val Acc: 93.56%\n",
            "Val Acc (top-2): 93.56% (c·∫£i thi·ªán: +-0.00%)\n",
            "\n",
            "Epoch 16/25\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3346/3346 [1:03:52<00:00,  1.15s/it, loss=0.1104, acc=92.25%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [10:15<00:00,  1.04s/it, loss=0.0871, acc=93.80%]\n",
            "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 591/591 [19:06<00:00,  1.94s/it, loss=0.0871, acc=93.80%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 0.1104 | Train Acc: 92.25%\n",
            "Val Loss: 0.0871 | Val Acc: 93.80%\n",
            "Val Acc (top-2): 93.80% (c·∫£i thi·ªán: +-0.00%)\n",
            "\n",
            "Epoch 17/25\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   2%|‚ñè         | 58/3346 [01:12<1:08:25,  1.25s/it, loss=0.1067, acc=92.56%]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mConfig.EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m train_loss, train_acc = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDEVICE\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Validation kh√¥ng d√πng top-2 logic (ƒë·ªÉ so s√°nh c√¥ng b·∫±ng)\u001b[39;00m\n\u001b[32m     17\u001b[39m val_loss, val_acc = validate(\n\u001b[32m     18\u001b[39m     model, val_loader, criterion, Config.DEVICE, use_top2=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     19\u001b[39m )\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, loader, criterion, optimizer, device)\u001b[39m\n\u001b[32m     40\u001b[39m total = \u001b[32m0\u001b[39m\n\u001b[32m     42\u001b[39m pbar = tqdm(loader, desc=\u001b[33m'\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vu The Van\\anaconda3\\envs\\httt\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vu The Van\\anaconda3\\envs\\httt\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vu The Van\\anaconda3\\envs\\httt\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vu The Van\\anaconda3\\envs\\httt\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vu The Van\\anaconda3\\envs\\httt\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mUnifiedDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     95\u001b[39m     image = Image.open(item[\u001b[33m'\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m'\u001b[39m]).convert(\u001b[33m'\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# Keep RGB for shapes\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     image = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpath\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.convert(\u001b[33m'\u001b[39m\u001b[33mL\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# Grayscale for digits/letters\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n\u001b[32m    100\u001b[39m     image = \u001b[38;5;28mself\u001b[39m.transform(image, is_shape=(item[\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m'\u001b[39m))\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vu The Van\\anaconda3\\envs\\httt\\Lib\\site-packages\\PIL\\Image.py:3513\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[32m   3512\u001b[39m     filename = os.fspath(fp)\n\u001b[32m-> \u001b[39m\u001b[32m3513\u001b[39m     fp = builtins.open(filename, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   3514\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3515\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "best_val_acc = 0.0\n",
        "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "for epoch in range(Config.EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{Config.EPOCHS}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    train_loss, train_acc = train_epoch(\n",
        "        model, train_loader, criterion, optimizer, Config.DEVICE\n",
        "    )\n",
        "    \n",
        "    # Validation kh√¥ng d√πng top-2 logic (ƒë·ªÉ so s√°nh c√¥ng b·∫±ng)\n",
        "    val_loss, val_acc = validate(\n",
        "        model, val_loader, criterion, Config.DEVICE, use_top2=False\n",
        "    )\n",
        "    \n",
        "    # Validation v·ªõi top-2 logic (ƒë·ªÉ xem c·∫£i thi·ªán)\n",
        "    val_loss_top2, val_acc_top2 = validate(\n",
        "        model, val_loader, criterion, Config.DEVICE, use_top2=True\n",
        "    )\n",
        "    \n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    \n",
        "    scheduler.step(val_acc)\n",
        "    \n",
        "    print(f\"\\nTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "    print(f\"Val Acc (top-2): {val_acc_top2:.2f}% (c·∫£i thi·ªán: +{val_acc_top2 - val_acc:.2f}%)\")\n",
        "    \n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_acc': val_acc,\n",
        "            'val_acc_top2': val_acc_top2,\n",
        "            'label_mapping': id_to_label,\n",
        "            'config': {\n",
        "                'epochs': Config.EPOCHS,\n",
        "                'batch_size': Config.BATCH_SIZE,\n",
        "                'lr': Config.LEARNING_RATE,\n",
        "                'input_size': Config.INPUT_SIZE,\n",
        "                'num_classes': Config.NUM_CLASSES\n",
        "            }\n",
        "        }, Config.MODEL_PATH)\n",
        "        print(f\"‚úÖ Saved best model: {Config.MODEL_PATH}\")\n",
        "        print(f\"   Val Acc: {val_acc:.2f}% | Val Acc (top-2): {val_acc_top2:.2f}%\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"TRAINING COMPLETED!\")\n",
        "print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
        "print(f\"Model saved: {Config.MODEL_PATH}\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Visualize Training History\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot Loss\n",
        "ax1.plot(history['train_loss'], label='Train Loss', marker='o')\n",
        "ax1.plot(history['val_loss'], label='Val Loss', marker='s')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training & Validation Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot Accuracy\n",
        "ax2.plot(history['train_acc'], label='Train Acc', marker='o')\n",
        "ax2.plot(history['val_acc'], label='Val Acc', marker='s')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy (%)')\n",
        "ax2.set_title('Training & Validation Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_history_improved.png', dpi=150, bbox_inches='tight')\n",
        "print(\"‚úÖ Saved training_history_improved.png\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Next Steps\n",
        "\n",
        "After training completes, run evaluation:\n",
        "\n",
        "```python\n",
        "# Run this in terminal or new notebook\n",
        "!python evaluate_model.py\n",
        "```\n",
        "\n",
        "Expected improvements:\n",
        "- **Digits accuracy**: Gi·∫£m l·ªói nh·∫ßm 4‚Üî9, 3‚Üî5, 6‚Üî5, 1‚Üî7\n",
        "- **Shapes accuracy**: Nh·∫≠n di·ªán ƒë∆∞·ª£c v·ªõi background v√† fill m√†u kh√°c nhau\n",
        "- **Letters**: Nh·∫≠n di·ªán ƒë∆∞·ª£c A-Z v√† a-z\n",
        "\n",
        "Then test on your sample image:\n",
        "\n",
        "```python\n",
        "!python pipeline.py --image Shapes_Classifier/Sample.png --output Sample_result_new.png\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**‚úÖ Training notebook created successfully!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "httt",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
