# Bá»˜ KHOA Há»ŒC VÃ€ CÃ”NG NGHá»†

## Há»ŒC VIá»†N CÃ”NG NGHá»† BÆ¯U CHÃNH VIá»„N THÃ”NG

---

# BÃO CÃO BÃ€I Táº¬P Lá»šN

**Há»ŒC PHáº¦N:** Xá»¬ LÃ áº¢NH

**Äá» tÃ i:** Nháº­n dáº¡ng chá»¯ sá»‘ vÃ  hÃ¬nh há»c Ä‘Æ¡n giáº£n báº±ng máº¡ng Neural

**Giáº£ng viÃªn:** TS. Pháº¡m HoÃ ng Viá»‡t

**NhÃ³m 25:**
- B22DCCN482 - Trá»‹nh Quang LÃ¢m
- B22DCCN434 - VÅ© NhÃ¢n KiÃªn  
- B22DCCN889 - VÅ© Tháº¿ VÄƒn

**Link sáº£n pháº©m:** [tvan16/Object_Detection_MNIST_SHAPE](https://github.com/tvan16/Object_Detection_MNIST_SHAPE)

**HÃ  Ná»™i, 11/2025**

---

## ğŸ“‹ Má»¤C Lá»¤C

1. [Giá»›i thiá»‡u](#1-giá»›i-thiá»‡u)
2. [Bá»‘i cáº£nh & Táº§m quan trá»ng](#2-bá»‘i-cáº£nh--táº§m-quan-trá»ng)
3. [Äá»™ng lá»±c chá»n MNIST má»Ÿ rá»™ng](#3-Ä‘á»™ng-lá»±c-chá»n-mnist-má»Ÿ-rá»™ng)
4. [Má»¥c tiÃªu nghiÃªn cá»©u](#4-má»¥c-tiÃªu-nghiÃªn-cá»©u)
5. [Pháº¡m vi thá»±c hiá»‡n](#5-pháº¡m-vi-thá»±c-hiá»‡n)
6. [Tá»•ng quan nghiÃªn cá»©u & CÃ´ng nghá»‡](#6-tá»•ng-quan-nghiÃªn-cá»©u--cÃ´ng-nghá»‡)
7. [Augmentation & Tiá»n xá»­ lÃ½](#7-augmentation--tiá»n-xá»­-lÃ½)
8. [Kiáº¿n trÃºc mÃ´ hÃ¬nh & CÃ´ng nghá»‡ huáº¥n luyá»‡n](#8-kiáº¿n-trÃºc-mÃ´-hÃ¬nh--cÃ´ng-nghá»‡-huáº¥n-luyá»‡n)
9. [MÃ´ táº£ táº­p dá»¯ liá»‡u](#9-mÃ´-táº£-táº­p-dá»¯-liá»‡u)
10. [Thá»±c nghiá»‡m](#10-thá»±c-nghiá»‡m)
11. [á»¨ng dá»¥ng & Triá»ƒn khai](#11-á»©ng-dá»¥ng--triá»ƒn-khai)
12. [HÆ°á»›ng cáº£i thiá»‡n](#12-hÆ°á»›ng-cáº£i-thiá»‡n)
13. [Káº¿t luáº­n](#13-káº¿t-luáº­n)
14. [TÃ i liá»‡u tham kháº£o](#14-tÃ i-liá»‡u-tham-kháº£o)

---

## 1. GIá»šI THIá»†U

Dá»± Ã¡n **"Unified Digits & Shapes Recognition System"** lÃ  má»™t há»‡ thá»‘ng nháº­n diá»‡n Ä‘á»‘i tÆ°á»£ng hoÃ n chá»‰nh, cÃ³ kháº£ nÄƒng phÃ¡t hiá»‡n vÃ  phÃ¢n loáº¡i Ä‘á»“ng thá»i **chá»¯ sá»‘ viáº¿t tay** (0-9) vÃ  **hÃ¬nh há»c** (9 loáº¡i) trong cÃ¹ng má»™t áº£nh. Há»‡ thá»‘ng sá»­ dá»¥ng kiáº¿n trÃºc **hai giai Ä‘oáº¡n** (Two-Stage): **Detection** Ä‘á»ƒ tÃ¬m vá»‹ trÃ­ cÃ¡c Ä‘á»‘i tÆ°á»£ng, sau Ä‘Ã³ **Classification** Ä‘á»ƒ nháº­n diá»‡n loáº¡i cá»§a tá»«ng Ä‘á»‘i tÆ°á»£ng.

## 2. Bá»I Cáº¢NH & Táº¦M QUAN TRá»ŒNG

Trong bá»‘i cáº£nh lÃ n sÃ³ng á»©ng dá»¥ng thá»‹ giÃ¡c mÃ¡y tÃ­nh Ä‘ang lan rá»™ng sang nhiá»u lÄ©nh vá»±c nhÆ° xe tá»± hÃ nh, sáº£n xuáº¥t thÃ´ng minh vÃ  cÃ´ng nghá»‡ giÃ¡o dá»¥c, yÃªu cáº§u vá» nhá»¯ng mÃ´ hÃ¬nh vá»«a nháº¹ vá»«a chÃ­nh xÃ¡c trá»Ÿ nÃªn cáº¥p thiáº¿t hÆ¡n bao giá» háº¿t. CÃ¡c há»‡ thá»‘ng triá»ƒn khai trong mÃ´i trÆ°á»ng thá»±c, Ä‘áº·c biá»‡t trÃªn thiáº¿t bá»‹ nhÃºng hoáº·c biÃªn, thÆ°á»ng bá»‹ giá»›i háº¡n tÃ i nguyÃªn tÃ­nh toÃ¡n nÃªn khÃ´ng thá»ƒ sá»­ dá»¥ng cÃ¡c kiáº¿n trÃºc quÃ¡ cá»“ng ká»nh, trong khi váº«n pháº£i Ä‘áº£m báº£o Ä‘á»™ tin cáº­y Ä‘á»§ cao cho cÃ¡c tÃ¡c vá»¥ nháº­n dáº¡ng vÃ  quyáº¿t Ä‘á»‹nh tá»± Ä‘á»™ng. Äiá»u nÃ y Ä‘áº·t ra nhu cáº§u nghiÃªn cá»©u cÃ¡c mÃ´ hÃ¬nh tá»‘i giáº£n nhÆ°ng hiá»‡u quáº£, cÃ³ kháº£ nÄƒng cÃ¢n báº±ng giá»¯a Ä‘á»™ phá»©c táº¡p, hiá»‡u nÄƒng vÃ  kháº£ nÄƒng triá»ƒn khai.

Táº­p dá»¯ liá»‡u MNIST truyá»n thá»‘ng tá»« lÃ¢u Ä‘Ã£ Ä‘Æ°á»£c xem nhÆ° chuáº©n má»±c cÆ¡ báº£n Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cÃ¡c thuáº­t toÃ¡n nháº­n dáº¡ng chá»¯ sá»‘ viáº¿t tay. Tuy nhiÃªn, bÃ i toÃ¡n gá»‘c chá»‰ dá»«ng láº¡i á»Ÿ viá»‡c phÃ¢n loáº¡i cÃ¡c chá»¯ sá»‘ Ä‘Æ¡n láº», trÃªn ná»n áº£nh Ä‘Æ¡n giáº£n, nÃªn chÆ°a pháº£n Ã¡nh Ä‘áº§y Ä‘á»§ nhá»¯ng thÃ¡ch thá»©c cá»§a cÃ¡c ká»‹ch báº£n thá»‹ giÃ¡c mÃ¡y tÃ­nh ngoÃ i Ä‘á»i thá»±c, nÆ¡i mÃ´ hÃ¬nh cáº§n xá»­ lÃ½ nhiá»u Ä‘á»‘i tÆ°á»£ng, bá»‘ cá»¥c phá»©c táº¡p vÃ  cÃ¡c má»‘i quan há»‡ khÃ´ng gian â€“ hÃ¬nh há»c giá»¯a cÃ¡c thÃ nh pháº§n trong áº£nh. Do Ä‘Ã³, MNIST á»Ÿ dáº¡ng nguyÃªn báº£n khÃ´ng cÃ²n Ä‘á»§ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ nÄƒng lá»±c cá»§a cÃ¡c kiáº¿n trÃºc hiá»‡n Ä‘áº¡i vá»‘n hÆ°á»›ng tá»›i á»©ng dá»¥ng trong mÃ´i trÆ°á»ng Ä‘á»™ng, Ä‘a Ä‘á»‘i tÆ°á»£ng.

Trong bá»‘i cáº£nh giÃ¡o dá»¥c vÃ  sÃ¡ng táº¡o sá»‘, thá»‹ giÃ¡c mÃ¡y tÃ­nh Ä‘Æ°á»£c sá»­ dá»¥ng cho nhiá»u nhiá»‡m vá»¥ nhÆ° theo dÃµi má»©c Ä‘á»™ tÆ°Æ¡ng tÃ¡c cá»§a ngÆ°á»i há»c, há»— trá»£ há»c táº­p cÃ¡ nhÃ¢n hÃ³a, xÃ¢y dá»±ng lá»›p há»c thÃ´ng minh hay táº¡o ná»™i dung há»c liá»‡u trá»±c quan. Nhá»¯ng há»‡ thá»‘ng nhÆ° váº­y thÆ°á»ng pháº£i xá»­ lÃ½ cÃ¡c cáº£nh phá»©c táº¡p vá»›i nhiá»u biá»ƒu tÆ°á»£ng, váº­t thá»ƒ há»c táº­p hoáº·c tÆ°Æ¡ng tÃ¡c cá»§a ngÆ°á»i há»c trong khÃ´ng gian lá»›p há»c váº­t lÃ½ hoáº·c áº£o. VÃ¬ váº­y, viá»‡c má»Ÿ rá»™ng bÃ i toÃ¡n tá»« nháº­n dáº¡ng chá»¯ sá»‘ Ä‘Æ¡n láº» sang phÃ¡t hiá»‡n vÃ  Ä‘á»‹nh vá»‹ nhiá»u Ä‘á»‘i tÆ°á»£ng trong má»™t khung hÃ¬nh cÃ³ Ã½ nghÄ©a thiáº¿t thá»±c, giÃºp mÃ´ hÃ¬nh tiáº¿n gáº§n hÆ¡n vá»›i cÃ¡c bÃ i toÃ¡n thá»±c táº¿ cá»§a EdTech.

Má»™t hÆ°á»›ng nghiÃªn cá»©u quan trá»ng lÃ  thiáº¿t káº¿ cÃ¡c biáº¿n thá»ƒ má»Ÿ rá»™ng cá»§a MNIST, trong Ä‘Ã³ cÃ¡c chá»¯ sá»‘ Ä‘Æ°á»£c káº¿t há»£p, sáº¯p xáº¿p theo cáº¥u trÃºc hÃ¬nh há»c hoáº·c Ä‘áº·t trong nhá»¯ng bá»‘ cá»¥c Ä‘a Ä‘á»‘i tÆ°á»£ng, nháº±m mÃ´ phá»ng cÃ¡c tÃ¬nh huá»‘ng chiáº¿n lÆ°á»£c trong mÃ´i trÆ°á»ng giÃ¡o dá»¥c vÃ  sÃ¡ng táº¡o. CÃ¡c táº­p dá»¯ liá»‡u nhÆ° váº­y cho phÃ©p Ä‘Ã¡nh giÃ¡ kháº£ nÄƒng cá»§a mÃ´ hÃ¬nh trong viá»‡c phÃ¡t hiá»‡n, phÃ¢n tÃ¡ch vÃ  hiá»ƒu quan há»‡ giá»¯a cÃ¡c Ä‘á»‘i tÆ°á»£ng, Ä‘á»“ng thá»i váº«n duy trÃ¬ kÃ­ch thÆ°á»›c dá»¯ liá»‡u vá»«a pháº£i Ä‘á»ƒ phÃ¹ há»£p cho viá»‡c thá»­ nghiá»‡m cÃ¡c kiáº¿n trÃºc nháº¹. Nhá» Ä‘Ã³, ngÆ°á»i nghiÃªn cá»©u cÃ³ thá»ƒ kháº£o sÃ¡t sÃ¢u hÆ¡n cÃ¡ch tá»‘i Æ°u máº¡ng nÆ¡-ron cho nhá»¯ng há»‡ thá»‘ng thá»‹ giÃ¡c mÃ¡y tÃ­nh Ã¡p dá»¥ng trong lá»›p há»c thÃ´ng minh, trÃ² chÆ¡i giÃ¡o dá»¥c hay cÃ´ng cá»¥ há»— trá»£ sÃ¡ng táº¡o, nÆ¡i rÃ ng buá»™c vá» tÃ i nguyÃªn vÃ  Ä‘á»™ trá»… lÃ  nhá»¯ng yáº¿u tá»‘ then chá»‘t.

## 3. Äá»˜NG Lá»°C CHá»ŒN MNIST Má» Rá»˜NG

Viá»‡c lá»±a chá»n MNIST lÃ m ná»n táº£ng Ä‘á»ƒ má»Ÿ rá»™ng xuáº¥t phÃ¡t tá»« chÃ­nh tÃ­nh biá»ƒu tÆ°á»£ng cá»§a bá»™ dá»¯ liá»‡u nÃ y trong cá»™ng Ä‘á»“ng há»c mÃ¡y vÃ  thá»‹ giÃ¡c mÃ¡y tÃ­nh. MNIST Ä‘Ã£ Ä‘Æ°á»£c nghiÃªn cá»©u ráº¥t ká»¹, cÃ³ tÃ i liá»‡u phong phÃº vÃ  nhiá»u vÃ­ dá»¥ mÃ£ nguá»“n, nÃªn viá»‡c tÃ¡i láº­p thÃ­ nghiá»‡m, so sÃ¡nh mÃ´ hÃ¬nh vÃ  Ä‘Ã¡nh giÃ¡ cáº£i tiáº¿n trá»Ÿ nÃªn thuáº­n lá»£i, Ä‘áº·c biá»‡t cho má»¥c Ä‘Ã­ch giáº£ng dáº¡y vÃ  thá»­ nghiá»‡m nhanh cÃ¡c Ã½ tÆ°á»Ÿng má»›i. Nhá» Ä‘Ã³, má»i thay Ä‘á»•i trÃªn MNIST má»Ÿ rá»™ng Ä‘á»u cÃ³ thá»ƒ Ä‘áº·t trong bá»‘i cáº£nh má»™t chuáº©n tham chiáº¿u quen thuá»™c, giÃºp káº¿t quáº£ nghiÃªn cá»©u dá»… diá»…n giáº£i vÃ  chia sáº» vá»›i cá»™ng Ä‘á»“ng.

BÃªn cáº¡nh Ä‘Ã³, cáº¥u trÃºc áº£nh Ä‘Æ¡n giáº£n (thang xÃ¡m 28Ã—28) cho phÃ©p dá»… dÃ ng tÃ¹y biáº¿n Ä‘á»ƒ káº¿t há»£p chá»¯ sá»‘ vá»›i cÃ¡c hÃ¬nh dáº¡ng hÃ¬nh há»c nhÆ° Ä‘Æ°á»ng tháº³ng, hÃ¬nh trÃ²n, hÃ¬nh Ä‘a giÃ¡c, hoáº·c sáº¯p xáº¿p nhiá»u chá»¯ sá»‘ trong cÃ¹ng má»™t khung hÃ¬nh, táº¡o nÃªn cÃ¡c "mini real-world" mÃ´ phá»ng báº£ng Ä‘iá»ƒm, Ã´ bÃ i táº­p hoáº·c giao diá»‡n trÃ² chÆ¡i cho há»c sinh. Nhá»¯ng bá»‘ cá»¥c nÃ y giÃºp chuyá»ƒn bÃ i toÃ¡n tá»« phÃ¢n loáº¡i Ä‘Æ¡n Ä‘á»‘i tÆ°á»£ng sang phÃ¡t hiá»‡n, Ä‘á»‹nh vá»‹ vÃ  hiá»ƒu quan há»‡ khÃ´ng gian giá»¯a nhiá»u Ä‘á»‘i tÆ°á»£ng, gáº§n hÆ¡n vá»›i cÃ¡c ká»‹ch báº£n EdTech vÃ  game hÃ³a há»c táº­p.

Má»™t Æ°u Ä‘iá»ƒm quan trá»ng khÃ¡c lÃ  MNIST cho phÃ©p kiá»ƒm soÃ¡t dá»¯ liá»‡u á»Ÿ má»©c cao, tá»« Ä‘Ã³ cÃ³ thá»ƒ chá»§ Ä‘á»™ng Ä‘Æ°a vÃ o cÃ¡c dáº¡ng nhiá»…u, chá»“ng chÃ©o Ä‘á»‘i tÆ°á»£ng, biáº¿n Ä‘á»•i affine (quay, tá»‹nh tiáº¿n, co giÃ£n, biáº¿n dáº¡ng phá»‘i cáº£nh) hay thay Ä‘á»•i Ä‘á»™ tÆ°Æ¡ng pháº£n vÃ  Ä‘á»™ sÃ¡ng. Kháº£ nÄƒng kiá»ƒm soÃ¡t nÃ y giÃºp xÃ¢y dá»±ng cÃ¡c bá»™ dá»¯ liá»‡u "cÃ³ chá»§ Ä‘Ã­ch", trong Ä‘Ã³ tá»«ng yáº¿u tá»‘ khÃ³ khÄƒn Ä‘Æ°á»£c gia tÄƒng cÃ³ káº¿ hoáº¡ch Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ Ä‘á»™ bá»n vá»¯ng cá»§a mÃ´ hÃ¬nh, Ä‘o lÆ°á»ng kháº£ nÄƒng khÃ¡i quÃ¡t hÃ³a trong Ä‘iá»u kiá»‡n gáº§n vá»›i tháº¿ giá»›i thá»±c nhÆ°ng váº«n an toÃ n, ráº» vÃ  dá»… triá»ƒn khai trong mÃ´i trÆ°á»ng giÃ¡o dá»¥c.

## 4. Má»¤C TIÃŠU NGHIÃŠN Cá»¨U

NghiÃªn cá»©u hÆ°á»›ng tá»›i xÃ¢y dá»±ng má»™t pipeline thá»‘ng nháº¥t cho bÃ i toÃ¡n MNIST má»Ÿ rá»™ng, bao trÃ¹m toÃ n bá»™ cÃ¡c bÆ°á»›c tá»« tiá»n xá»­ lÃ½ dá»¯ liá»‡u, táº¡o máº«u Ä‘áº¿n huáº¥n luyá»‡n vÃ  suy luáº­n, vá»›i kháº£ nÄƒng phÃ¡t hiá»‡n Ä‘á»“ng thá»i cáº£ chá»¯ sá»‘ vÃ  cÃ¡c hÃ¬nh dáº¡ng hÃ¬nh há»c trong cÃ¹ng má»™t khung hÃ¬nh. Pipeline nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ sao cho cÃ³ thá»ƒ Ã¡p dá»¥ng láº¡i dá»… dÃ ng cho cÃ¡c biáº¿n thá»ƒ dá»¯ liá»‡u khÃ¡c nhau, nhÆ°ng váº«n giá»¯ cÃ¡ch tá»• chá»©c rÃµ rÃ ng giá»¯a cÃ¡c khá»‘i chá»©c nÄƒng nhÆ° táº¡o dá»¯ liá»‡u, huáº¥n luyá»‡n mÃ´ hÃ¬nh vÃ  Ä‘Ã¡nh giÃ¡ káº¿t quáº£. Má»™t má»¥c tiÃªu quan trá»ng lÃ  duy trÃ¬ sá»± cÃ¢n báº±ng há»£p lÃ½ giá»¯a Ä‘á»™ chÃ­nh xÃ¡c vÃ  tá»‘c Ä‘á»™, nháº±m Ä‘áº£m báº£o mÃ´ hÃ¬nh khÃ´ng chá»‰ Ä‘áº¡t hiá»‡u nÄƒng nháº­n dáº¡ng tá»‘t trÃªn bá»™ dá»¯ liá»‡u MNIST má»Ÿ rá»™ng mÃ  cÃ²n cÃ³ Ä‘á»™ trá»… tháº¥p, phÃ¹ há»£p vá»›i yÃªu cáº§u triá»ƒn khai trong cÃ¡c há»‡ thá»‘ng thá»±c táº¿ nhÆ° á»©ng dá»¥ng giÃ¡o dá»¥c tÆ°Æ¡ng tÃ¡c hoáº·c trÃ² chÆ¡i há»c táº­p. Trong bá»‘i cáº£nh tÃ i nguyÃªn tÃ­nh toÃ¡n bá»‹ giá»›i háº¡n trÃªn thiáº¿t bá»‹ biÃªn, viá»‡c tá»‘i Æ°u mÃ´ hÃ¬nh vÃ  pipeline Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c sá»± Ä‘Ã¡nh Ä‘á»•i hiá»‡u quáº£ giá»¯a chi phÃ­ tÃ­nh toÃ¡n vÃ  cháº¥t lÆ°á»£ng dá»± Ä‘oÃ¡n lÃ  tiÃªu chÃ­ then chá»‘t.

BÃªn cáº¡nh Ä‘Ã³, nghiÃªn cá»©u Ä‘áº·t má»¥c tiÃªu cung cáº¥p bá»™ cÃ´ng cá»¥ cÃ³ kháº£ nÄƒng tÃ¡i láº­p cao dÆ°á»›i dáº¡ng script vÃ  notebook, cho phÃ©p ngÆ°á»i dÃ¹ng dá»… dÃ ng táº£i dá»¯ liá»‡u, huáº¥n luyá»‡n láº¡i mÃ´ hÃ¬nh, Ä‘iá»u chá»‰nh siÃªu tham sá»‘ vÃ  Ä‘Ã¡nh giÃ¡ káº¿t quáº£. CÃ¡c tÃ i liá»‡u vÃ  mÃ£ nguá»“n Ä‘i kÃ¨m Ä‘Æ°á»£c tá»• chá»©c theo hÆ°á»›ng thÃ¢n thiá»‡n vá»›i cá»™ng Ä‘á»“ng, giÃºp sinh viÃªn, nhÃ  nghiÃªn cá»©u hoáº·c nhÃ  phÃ¡t triá»ƒn cÃ³ thá»ƒ nhanh chÃ³ng má»Ÿ rá»™ng, so sÃ¡nh vÃ  tÃ­ch há»£p pipeline nÃ y vÃ o nhá»¯ng bÃ i toÃ¡n thá»‹ giÃ¡c mÃ¡y tÃ­nh khÃ¡c nhau trong mÃ´i trÆ°á»ng giÃ¡o dá»¥c vÃ  sÃ¡ng táº¡o.

## 5. PHáº M VI THá»°C HIá»†N

Trong khuÃ´n khá»• nghiÃªn cá»©u nÃ y, dá»¯ liá»‡u Ä‘áº§u vÃ o Ä‘Æ°á»£c nhÃ³m tá»± sinh vÃ  tÃ¡i cáº¥u trÃºc, Ä‘Æ°á»£c quáº£n lÃ½ táº­p trung trong thÆ° má»¥c dataset/, nháº±m Ä‘áº£m báº£o kháº£ nÄƒng kiá»ƒm soÃ¡t tá»‘t quÃ¡ trÃ¬nh táº¡o máº«u, gáº¯n nhÃ£n vÃ  tÃ¡i láº­p thÃ­ nghiá»‡m. CÃ¡ch tiáº¿p cáº­n nÃ y giÃºp dá»… dÃ ng Ä‘iá»u chá»‰nh cÃ¡c tham sá»‘ sinh dá»¯ liá»‡u nhÆ° phÃ¢n bá»‘ vá»‹ trÃ­, má»©c nhiá»…u hay máº­t Ä‘á»™ Ä‘á»‘i tÆ°á»£ng, Ä‘á»“ng thá»i thuáº­n lá»£i cho viá»‡c chia tÃ¡ch táº­p huáº¥n luyá»‡n, kiá»ƒm thá»­ vÃ  Ä‘Ã¡nh giÃ¡.

Äá» tÃ i chá»‰ táº­p trung vÃ o cÃ¡c hÃ¬nh dáº¡ng hÃ¬nh há»c cÆ¡ báº£n nhÆ° hÃ¬nh trÃ²n, tam giÃ¡c, hÃ¬nh vuÃ´ng (vÃ  má»™t sá»‘ biáº¿n thá»ƒ Ä‘Æ¡n giáº£n náº¿u cÃ³) cÃ¹ng vá»›i cÃ¡c chá»¯ sá»‘ 0â€“9, qua Ä‘Ã³ giá»¯ cho khÃ´ng gian lá»›p nhÃ£n Ä‘á»§ Ä‘Æ¡n giáº£n Ä‘á»ƒ phÃ¢n tÃ­ch nhÆ°ng váº«n Ä‘á»§ Ä‘a dáº¡ng Ä‘á»ƒ mÃ´ phá»ng cÃ¡c ká»‹ch báº£n Ä‘a Ä‘á»‘i tÆ°á»£ng. Viá»‡c giá»›i háº¡n nÃ y giÃºp lÃ m rÃµ tÃ¡c Ä‘á»™ng cá»§a thiáº¿t káº¿ mÃ´ hÃ¬nh vÃ  pipeline lÃªn bÃ i toÃ¡n phÃ¡t hiá»‡n káº¿t há»£p shape + digit, trÃ¡nh bá»‹ nhiá»…u bá»Ÿi quÃ¡ nhiá»u loáº¡i Ä‘á»‘i tÆ°á»£ng khÃ¡c nhau.

Vá» máº·t triá»ƒn khai, nghiÃªn cá»©u giáº£ Ä‘á»‹nh mÃ´i trÆ°á»ng tÃ­nh toÃ¡n lÃ  cÃ¡c GPU phá»• thÃ´ng thÆ°á»ng gáº·p trong phÃ²ng lab hoáº·c mÃ¡y tráº¡m, khÃ´ng Ä‘i sÃ¢u vÃ o cÃ¡c tá»‘i Æ°u hÃ³a pháº§n cá»©ng chuyÃªn biá»‡t cho IoT hoáº·c thiáº¿t bá»‹ edge. Nhá»¯ng váº¥n Ä‘á» nhÆ° nÃ©n mÃ´ hÃ¬nh cá»±c máº¡nh, triá»ƒn khai trÃªn vi Ä‘iá»u khiá»ƒn, hoáº·c tÃ­ch há»£p vá»›i há»‡ thá»‘ng nhÃºng chá»‰ Ä‘Æ°á»£c Ä‘á» cáº­p á»Ÿ má»©c Ä‘á»‹nh hÆ°á»›ng tÆ°Æ¡ng lai, nháº±m giá»¯ pháº¡m vi thá»±c hiá»‡n phÃ¹ há»£p vá»›i nguá»“n lá»±c vÃ  má»¥c tiÃªu Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh trong bá»‘i cáº£nh há»c thuáº­t.

### Má»¥c tiÃªu

- ğŸ¯ XÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh thá»‘ng nháº¥t cÃ³ thá»ƒ nháº­n diá»‡n cáº£ chá»¯ sá»‘ vÃ  hÃ¬nh há»c trong cÃ¹ng má»™t pipeline
- ğŸ¯ Äáº¡t Ä‘á»™ chÃ­nh xÃ¡c cao (>99%) trÃªn cáº£ hai loáº¡i Ä‘á»‘i tÆ°á»£ng
- ğŸ¯ Tá»‘i Æ°u tá»‘c Ä‘á»™ inference Ä‘á»ƒ cÃ³ thá»ƒ Ã¡p dá»¥ng trong thá»±c táº¿
- ğŸ¯ Há»— trá»£ nhiá»u phÆ°Æ¡ng phÃ¡p detection linh hoáº¡t (Traditional CV, CRAFT, Hybrid)
- ğŸ¯ TÃ­ch há»£p MQTT Ä‘á»ƒ xá»­ lÃ½ real-time tá»« frontend

### á»¨ng dá»¥ng thá»±c táº¿

- ğŸ“ **Nháº­n diá»‡n chá»¯ sá»‘ viáº¿t tay**: Äá»c sá»‘ tá»« biá»ƒu máº«u, hÃ³a Ä‘Æ¡n, chá»©ng tá»«
- ğŸ”· **PhÃ¢n loáº¡i hÃ¬nh há»c**: PhÃ¢n tÃ­ch hÃ¬nh dáº¡ng trong áº£nh ká»¹ thuáº­t, báº£n váº½
- ğŸ“ **GiÃ¡o dá»¥c**: Há»— trá»£ há»c sinh nháº­n diá»‡n sá»‘ vÃ  hÃ¬nh há»c
- ğŸ­ **Tá»± Ä‘á»™ng hÃ³a**: Xá»­ lÃ½ áº£nh trong dÃ¢y chuyá»n sáº£n xuáº¥t
- ğŸ“± **Mobile Apps**: TÃ­ch há»£p vÃ o á»©ng dá»¥ng di Ä‘á»™ng Ä‘á»ƒ nháº­n diá»‡n real-time

## ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

### Pipeline tá»•ng quan

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input Image â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Stage 1: Object Detection     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ Traditional CV Detector  â”‚  â”‚
â”‚   â”‚ (Contour-based)          â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ CRAFT Detector           â”‚  â”‚
â”‚   â”‚ (Text/Character)         â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ Hybrid Detector          â”‚  â”‚
â”‚   â”‚ (CV + CRAFT combined)    â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Bounding Boxes (x, y, w, h)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Stage 2: Classification       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ EfficientNet-B0          â”‚  â”‚
â”‚   â”‚ (19 classes)             â”‚  â”‚
â”‚   â”‚ - 10 digits (0-9)        â”‚  â”‚
â”‚   â”‚ - 9 shapes               â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Labels + Confidences
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Stage 3: Post-processing      â”‚
â”‚   - Filter by target (digits/  â”‚
â”‚     shapes/all)                 â”‚
â”‚   - Sort by reading order       â”‚
â”‚   - Visualize annotations       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Output:                        â”‚
â”‚   - Annotated Image (PNG)       â”‚
â”‚   - JSON Results                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### CÃ¡c thÃ nh pháº§n chÃ­nh

#### 1. **Detection Module** (`detect_objects.py`)

**Traditional CV Detector:**
- Sá»­ dá»¥ng OpenCV Ä‘á»ƒ tÃ¬m contours
- Preprocessing: Denoising, CLAHE, Illumination correction
- Adaptive thresholding Ä‘á»ƒ tÃ¡ch foreground/background
- Filter theo area, aspect ratio Ä‘á»ƒ loáº¡i bá» noise

**CRAFT Detector:**
- Deep learning model Ä‘á»ƒ detect text/characters
- Pre-trained trÃªn MLT dataset (25k images)
- Tá»‘t cho viá»‡c detect chá»¯ sá»‘ vÃ  kÃ½ tá»±

**Hybrid Detector:**
- Káº¿t há»£p Traditional CV + CRAFT
- CRAFT detect digits, Traditional CV detect shapes
- Merge vÃ  deduplicate káº¿t quáº£
- Tá»‘i Æ°u cho áº£nh cÃ³ cáº£ digits vÃ  shapes

#### 2. **Classification Module** (`train_unified_classifier.py`)

**Model Architecture:**
- **Backbone**: EfficientNet-B0 (pre-trained trÃªn ImageNet)
- **Input**: 128x128 RGB images (grayscale converted)
- **Output**: 19 classes (10 digits + 9 shapes)
- **Augmentation**: Rotation, Affine, Perspective, ColorJitter (balanced Ä‘á»ƒ giá»¯ shape edges)

**Training Process:**
- Dataset: ~100,000 images (MNIST + Shapes)
- Epochs: 20
- Optimizer: Adam (lr=1e-4)
- Loss: CrossEntropy
- Validation accuracy: ~99.14%

#### 3. **Pipeline Module** (`pipeline.py`)

**Chá»©c nÄƒng:**
- Káº¿t há»£p Detection + Classification
- Filter theo target classes (digits/shapes/all)
- Sort detections theo reading order (top-to-bottom, left-to-right)
- Visualize vá»›i bounding boxes vÃ  labels
- Generate synthetic test images
- MQTT integration cho real-time processing

#### 4. **MQTT Integration**

**Topics:**
- `image/create`: Request generate áº£nh synthetic
- `image/input/create`: Response vá»›i áº£nh Ä‘Ã£ generate
- `image/input`: Request xá»­ lÃ½ áº£nh
- `image/output`: Response vá»›i káº¿t quáº£ detection

**Flow:**
```
Frontend â†’ image/create â†’ AI generate â†’ image/input/create â†’ Frontend
Frontend â†’ image/input â†’ AI process â†’ image/output â†’ Frontend
```

## ğŸ”¬ CÃ´ng nghá»‡ vÃ  phÆ°Æ¡ng phÃ¡p

### Deep Learning

- **EfficientNet-B0**: CNN architecture tá»‘i Æ°u vá» accuracy/efficiency
- **Transfer Learning**: Pre-trained trÃªn ImageNet, fine-tune trÃªn custom dataset
- **Data Augmentation**: TÄƒng diversity cá»§a training data

### Computer Vision

- **Contour Detection**: TÃ¬m boundaries cá»§a objects
- **Adaptive Thresholding**: Tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh threshold theo local regions
- **CLAHE**: Contrast Limited Adaptive Histogram Equalization
- **Morphological Operations**: LÃ m sáº¡ch vÃ  tÃ¡ch objects

### Text Detection

- **CRAFT**: Character Region Awareness For Text detection
- **Region Proposal**: TÃ¬m regions cÃ³ kháº£ nÄƒng chá»©a text
- **Link Prediction**: Káº¿t ná»‘i cÃ¡c characters thÃ nh words

### Preprocessing

- **Denoising**: Loáº¡i bá» noise trong áº£nh
- **Contrast Enhancement**: TÄƒng Ä‘á»™ tÆ°Æ¡ng pháº£n
- **Illumination Correction**: Chuáº©n hÃ³a Ã¡nh sÃ¡ng
- **Normalization**: Chuáº©n hÃ³a pixel values

---

## ğŸ“š CHI TIáº¾T Ká»¸ THUáº¬T VÃ€ GIáº¢I THÃCH

### 1. Táº I SAO DÃ™NG PRETRAINED MODEL? PRETRAINED MODEL CÃ“ GÃŒ Äáº¶C BIá»†T?

#### 1.1. LÃ½ do sá»­ dá»¥ng Pretrained Model

**Transfer Learning - Há»c chuyá»ƒn giao:**
- **Äá»‹nh nghÄ©a**: Sá»­ dá»¥ng kiáº¿n thá»©c Ä‘Ã£ há»c tá»« má»™t task lá»›n (ImageNet) Ä‘á»ƒ Ã¡p dá»¥ng vÃ o task má»›i (nháº­n diá»‡n digits/shapes)
- **Lá»£i Ã­ch**:
  1. **Tiáº¿t kiá»‡m thá»i gian training**: Thay vÃ¬ train tá»« Ä‘áº§u (cáº§n hÃ ng triá»‡u áº£nh vÃ  hÃ ng tuáº§n), chá»‰ cáº§n fine-tune vÃ i giá»
  2. **Cáº§n Ã­t dá»¯ liá»‡u hÆ¡n**: Vá»›i pretrained model, chá»‰ cáº§n ~100K áº£nh thay vÃ¬ hÃ ng triá»‡u áº£nh
  3. **Äáº¡t accuracy cao hÆ¡n**: Model Ä‘Ã£ há»c Ä‘Æ°á»£c cÃ¡c features cÆ¡ báº£n (edges, textures, shapes) tá»« ImageNet
  4. **TrÃ¡nh overfitting**: Vá»›i dataset nhá», train tá»« Ä‘áº§u dá»… bá»‹ overfitting

**So sÃ¡nh:**
```
Train tá»« Ä‘áº§u:  100K áº£nh â†’ Accuracy ~85-90% (cáº§n nhiá»u epochs)
Pretrained:    100K áº£nh â†’ Accuracy ~99% (chá»‰ cáº§n 20 epochs)
```

#### 1.2. EfficientNet-B0 Pretrained trÃªn ImageNet - Äáº·c Ä‘iá»ƒm gÃ¬?

**ImageNet Dataset:**
- **Quy mÃ´**: 1.2 triá»‡u áº£nh, 1000 classes
- **Äa dáº¡ng**: Äá»™ng váº­t, Ä‘á»“ váº­t, thá»±c pháº©m, phÆ°Æ¡ng tiá»‡n, v.v.
- **Cháº¥t lÆ°á»£ng**: ÄÆ°á»£c label cáº©n tháº­n, Ä‘a dáº¡ng vá» gÃ³c chá»¥p, Ã¡nh sÃ¡ng, background

**EfficientNet-B0 Architecture:**
- **Compound Scaling**: Tá»‘i Æ°u Ä‘á»“ng thá»i depth, width, vÃ  resolution
- **MobileNetV2 blocks**: Depthwise separable convolutions (hiá»‡u quáº£ hÆ¡n)
- **Squeeze-and-Excitation**: Attention mechanism Ä‘á»ƒ táº­p trung vÃ o features quan trá»ng
- **Swish activation**: f(x) = x * sigmoid(x) - tá»‘t hÆ¡n ReLU

**Features Ä‘Ã£ há»c Ä‘Æ°á»£c tá»« ImageNet:**
1. **Low-level features** (táº§ng Ä‘áº§u):
   - Edge detection (phÃ¡t hiá»‡n cáº¡nh)
   - Texture patterns (máº«u káº¿t cáº¥u)
   - Color blobs (vÃ¹ng mÃ u)
   
2. **Mid-level features** (táº§ng giá»¯a):
   - Shapes vÃ  contours (hÃ¬nh dáº¡ng vÃ  Ä‘Æ°á»ng viá»n)
   - Parts of objects (bá»™ pháº­n Ä‘á»‘i tÆ°á»£ng)
   - Spatial relationships (má»‘i quan há»‡ khÃ´ng gian)

3. **High-level features** (táº§ng cuá»‘i):
   - Object recognition (nháº­n diá»‡n Ä‘á»‘i tÆ°á»£ng)
   - Scene understanding (hiá»ƒu cáº£nh)

**Táº¡i sao phÃ¹ há»£p vá»›i digits/shapes?**
- Digits vÃ  shapes cÅ©ng lÃ  **objects** vá»›i **edges, contours, shapes**
- Model Ä‘Ã£ biáº¿t cÃ¡ch nháº­n diá»‡n **geometric patterns** tá»« ImageNet
- Chá»‰ cáº§n fine-tune classifier layer Ä‘á»ƒ phÃ¢n biá»‡t 19 classes cá»¥ thá»ƒ

#### 1.3. Fine-tuning Process

**CÃ¡ch fine-tune:**
```python
# 1. Load pretrained weights
model = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)

# 2. Thay Ä‘á»•i classifier layer (tá»« 1000 classes â†’ 19 classes)
num_features = model.classifier[1].in_features  # 1280 features
model.classifier[1] = nn.Linear(num_features, 19)  # 19 classes

# 3. Train vá»›i learning rate nhá» (1e-4) Ä‘á»ƒ khÃ´ng phÃ¡ vá»¡ pretrained weights
optimizer = optim.Adam(model.parameters(), lr=1e-4)
```

**Táº¡i sao learning rate nhá»?**
- Pretrained weights Ä‘Ã£ tá»‘t, chá»‰ cáº§n Ä‘iá»u chá»‰nh nháº¹
- Learning rate lá»›n sáº½ "xÃ³a" kiáº¿n thá»©c Ä‘Ã£ há»c tá»« ImageNet
- Learning rate nhá» giÃºp model há»c thÃªm features má»›i mÃ  khÃ´ng quÃªn cÅ©

---

### 2. CHI TIáº¾T Vá»€ CONTOUR DETECTION VÃ€ BOUNDING BOX

#### 2.1. Quy trÃ¬nh Contour Detection

**BÆ°á»›c 1: Preprocessing**
```python
# Convert to grayscale
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# Gaussian Blur Ä‘á»ƒ giáº£m noise
blurred = cv2.GaussianBlur(gray, (5, 5), 0)
# Kernel size (5,5): cá»­a sá»• 5x5 pixels
# Sigma=0: tá»± Ä‘á»™ng tÃ­nh tá»« kernel size
```
**Táº¡i sao blur?**
- Loáº¡i bá» noise nhá» (pixels lá»—i, artifacts)
- LÃ m má»‹n áº£nh Ä‘á»ƒ thresholding tá»‘t hÆ¡n
- Giáº£m false positives tá»« noise

**BÆ°á»›c 2: Adaptive Thresholding**
```python
binary = cv2.adaptiveThreshold(
    blurred, 255,                          # Max value
    cv2.ADAPTIVE_THRESH_GAUSSIAN_C,        # Method
    cv2.THRESH_BINARY_INV,                 # Invert (objects = white)
    11,                                     # Block size (11x11)
    2                                       # C constant
)
```

**Adaptive Threshold vs Global Threshold:**
- **Global Threshold**: DÃ¹ng 1 giÃ¡ trá»‹ cho toÃ n áº£nh â†’ khÃ´ng tá»‘t vá»›i Ã¡nh sÃ¡ng khÃ´ng Ä‘á»u
- **Adaptive Threshold**: TÃ­nh threshold riÃªng cho tá»«ng vÃ¹ng 11x11 pixels

**CÃ¡ch hoáº¡t Ä‘á»™ng:**
1. Chia áº£nh thÃ nh cÃ¡c block 11x11 pixels
2. TÃ­nh mean cá»§a má»—i block
3. Threshold = mean - C (C=2)
4. Náº¿u pixel > threshold â†’ white (255), ngÆ°á»£c láº¡i â†’ black (0)

**Táº¡i sao THRESH_BINARY_INV?**
- Objects (digits/shapes) thÆ°á»ng tá»‘i trÃªn ná»n sÃ¡ng
- Invert Ä‘á»ƒ objects thÃ nh white (dá»… tÃ¬m contours)

**BÆ°á»›c 3: Morphological Operations**
```python
kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
morph = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
```

**Morphological Closing:**
- **Dilation** (giÃ£n ná»Ÿ) â†’ **Erosion** (co láº¡i)
- **Má»¥c Ä‘Ã­ch**: ÄÃ³ng cÃ¡c lá»— há»•ng nhá» trong objects, ná»‘i cÃ¡c pháº§n bá»‹ Ä‘á»©t

**VÃ­ dá»¥:**
```
TrÆ°á»›c:  [1 0 1]  â†’  Sau:  [1 1 1]
        [0 0 0]           [0 0 0]
        [1 0 1]           [1 1 1]
```
- ÄÃ³ng khoáº£ng trá»‘ng giá»¯a cÃ¡c pháº§n cá»§a chá»¯ sá»‘ "8"

**BÆ°á»›c 4: Find Contours**
```python
contours, _ = cv2.findContours(
    morph, 
    cv2.RETR_EXTERNAL,      # Chá»‰ láº¥y contours ngoÃ i cÃ¹ng
    cv2.CHAIN_APPROX_SIMPLE # NÃ©n contours (chá»‰ giá»¯ Ä‘iá»ƒm gÃ³c)
)
```

**RETR_EXTERNAL vs RETR_TREE:**
- **RETR_EXTERNAL**: Chá»‰ láº¥y contours ngoÃ i cÃ¹ng (khÃ´ng láº¥y lá»— há»•ng bÃªn trong)
- **RETR_TREE**: Láº¥y táº¥t cáº£ contours (bao gá»“m cáº£ lá»— há»•ng)

**CHAIN_APPROX_SIMPLE vs CHAIN_APPROX_NONE:**
- **SIMPLE**: NÃ©n contours, chá»‰ giá»¯ Ä‘iá»ƒm gÃ³c â†’ tiáº¿t kiá»‡m memory
- **NONE**: Giá»¯ táº¥t cáº£ Ä‘iá»ƒm â†’ chÃ­nh xÃ¡c hÆ¡n nhÆ°ng tá»‘n memory

**BÆ°á»›c 5: Extract Bounding Boxes**
```python
for contour in contours:
    area = cv2.contourArea(contour)
    
    # Filter by area
    if min_area < area < max_area:
        x, y, w, h = cv2.boundingRect(contour)
        
        # Filter by aspect ratio
        aspect_ratio = w / float(h)
        if min_ratio < aspect_ratio < max_ratio:
            bboxes.append((x, y, w, h))
```

**cv2.boundingRect(contour):**
- TÃ¬m hÃ¬nh chá»¯ nháº­t nhá» nháº¥t bao quanh contour
- Tráº£ vá»: (x, y, w, h)
  - x, y: Tá»a Ä‘á»™ gÃ³c trÃªn-trÃ¡i
  - w, h: Chiá»u rá»™ng vÃ  chiá»u cao

**Filtering:**
- **Area filter**: Loáº¡i bá» noise nhá» (< min_area) vÃ  objects quÃ¡ lá»›n (> max_area)
- **Aspect ratio filter**: Loáº¡i bá» objects quÃ¡ dáº¹t hoáº·c quÃ¡ cao (khÃ´ng pháº£i digits/shapes)

**VÃ­ dá»¥ tham sá»‘:**
```python
min_area = 100      # Loáº¡i bá» noise < 100 pixelsÂ²
max_area = 50000    # Loáº¡i bá» objects > 50000 pixelsÂ²
aspect_ratio = (0.3, 3.0)  # Cháº¥p nháº­n width/height tá»« 0.3 Ä‘áº¿n 3.0
```

#### 2.2. Bounding Box Format

**Format: (x, y, w, h)**
- **x, y**: Tá»a Ä‘á»™ gÃ³c trÃªn-trÃ¡i cá»§a bounding box
- **w, h**: Chiá»u rá»™ng vÃ  chiá»u cao

**VÃ­ dá»¥:**
```
Image: 800x600
Bounding box: (100, 50, 200, 150)
â†’ x=100, y=50, w=200, h=150
â†’ GÃ³c trÃªn-trÃ¡i: (100, 50)
â†’ GÃ³c dÆ°á»›i-pháº£i: (300, 200)
```

**Táº¡i sao dÃ¹ng (x, y, w, h) thay vÃ¬ (x1, y1, x2, y2)?**
- Dá»… tÃ­nh toÃ¡n area: `area = w * h`
- Dá»… resize: `new_w = w * scale`
- Chuáº©n OpenCV

---

### 3. CHI TIáº¾T Vá»€ DATA AUGMENTATION

#### 3.1. Táº¡i sao cáº§n Data Augmentation?

**Váº¥n Ä‘á»:**
- Dataset cÃ³ háº¡n (100K áº£nh)
- Model cáº§n há»c Ä‘Æ°á»£c tÃ­nh **invariant** (báº¥t biáº¿n) vá»›i:
  - Rotation (xoay)
  - Translation (dá»‹ch chuyá»ƒn)
  - Scale (thay Ä‘á»•i kÃ­ch thÆ°á»›c)
  - Lighting (Ã¡nh sÃ¡ng)
  - Perspective (gÃ³c nhÃ¬n)

**Giáº£i phÃ¡p: Data Augmentation**
- Táº¡o thÃªm dá»¯ liá»‡u tá»« dá»¯ liá»‡u cÃ³ sáºµn
- TÄƒng diversity mÃ  khÃ´ng cáº§n thu tháº­p thÃªm áº£nh
- Giáº£m overfitting

#### 3.2. CÃ¡c phÆ°Æ¡ng phÃ¡p Augmentation Ä‘Æ°á»£c sá»­ dá»¥ng

**1. RandomRotation (30Â°)**
```python
transforms.RandomRotation(30)
```
**Má»¥c Ä‘Ã­ch:**
- Model há»c Ä‘Æ°á»£c digits/shapes á»Ÿ má»i gÃ³c xoay
- Thá»±c táº¿: áº¢nh cÃ³ thá»ƒ bá»‹ xoay khi scan/chá»¥p

**Táº¡i sao 30Â°?**
- QuÃ¡ lá»›n (>45Â°): Digits/shapes khÃ³ nháº­n diá»‡n
- QuÃ¡ nhá» (<15Â°): KhÃ´ng Ä‘á»§ diversity
- 30Â°: CÃ¢n báº±ng tá»‘t

**VÃ­ dá»¥:**
```
Chá»¯ sá»‘ "6" xoay 30Â° â†’ váº«n lÃ  "6"
HÃ¬nh vuÃ´ng xoay 30Â° â†’ thÃ nh hÃ¬nh thoi (váº«n nháº­n diá»‡n Ä‘Æ°á»£c)
```

**2. RandomAffine (Translation)**
```python
transforms.RandomAffine(
    degrees=0,              # KhÃ´ng xoay (Ä‘Ã£ cÃ³ RandomRotation)
    translate=(0.15, 0.15), # Dá»‹ch 15% theo x vÃ  y
    scale=(0.8, 1.2),       # Scale tá»« 80% Ä‘áº¿n 120%
    shear=10                # Shear 10Â°
)
```

**Translation (0.15, 0.15):**
- Dá»‹ch chuyá»ƒn object 15% theo chiá»u ngang vÃ  dá»c
- Má»¥c Ä‘Ã­ch: Model há»c Ä‘Æ°á»£c object á»Ÿ má»i vá»‹ trÃ­ trong áº£nh

**Scale (0.8, 1.2):**
- Thay Ä‘á»•i kÃ­ch thÆ°á»›c tá»« 80% Ä‘áº¿n 120%
- Má»¥c Ä‘Ã­ch: Model há»c Ä‘Æ°á»£c object á»Ÿ má»i kÃ­ch thÆ°á»›c

**Shear (10Â°):**
- Biáº¿n dáº¡ng hÃ¬nh há»c (nghiÃªng)
- Má»¥c Ä‘Ã­ch: MÃ´ phá»ng gÃ³c chá»¥p nghiÃªng

**3. RandomPerspective**
```python
transforms.RandomPerspective(distortion_scale=0.2, p=0.5)
```

**Perspective Transformation:**
- MÃ´ phá»ng gÃ³c nhÃ¬n 3D (nhÆ° nhÃ¬n tá»« gÃ³c nghiÃªng)
- distortion_scale=0.2: Äá»™ biáº¿n dáº¡ng 20%
- p=0.5: Chá»‰ Ã¡p dá»¥ng 50% áº£nh (khÃ´ng quÃ¡ máº¡nh)

**VÃ­ dá»¥:**
```
HÃ¬nh vuÃ´ng nhÃ¬n tá»« trÃªn â†’ HÃ¬nh thang (perspective)
```

**4. ColorJitter**
```python
transforms.ColorJitter(brightness=0.3, contrast=0.3)
```

**Brightness (0.3):**
- Thay Ä‘á»•i Ä‘á»™ sÃ¡ng Â±30%
- Má»¥c Ä‘Ã­ch: Model há»c Ä‘Æ°á»£c vá»›i má»i Ä‘iá»u kiá»‡n Ã¡nh sÃ¡ng

**Contrast (0.3):**
- Thay Ä‘á»•i Ä‘á»™ tÆ°Æ¡ng pháº£n Â±30%
- Má»¥c Ä‘Ã­ch: Model há»c Ä‘Æ°á»£c vá»›i má»i Ä‘á»™ tÆ°Æ¡ng pháº£n

**Táº¡i sao khÃ´ng dÃ¹ng Saturation/Hue?**
- Digits/shapes lÃ  grayscale â†’ khÃ´ng cáº§n
- Chá»‰ cáº§n brightness vÃ  contrast

**5. Resize (128x128)**
```python
transforms.Resize((128, 128))
```

**Táº¡i sao 128x128?**
- **TÄƒng tá»« 64x64**: Äá»ƒ phÃ¢n biá»‡t tá»‘t hÆ¡n cÃ¡c shapes cÃ³ nhiá»u cáº¡nh (Nonagon, Octagon)
- **KhÃ´ng quÃ¡ lá»›n**: 128x128 Ä‘á»§ Ä‘á»ƒ nháº­n diá»‡n, khÃ´ng tá»‘n quÃ¡ nhiá»u memory
- **EfficientNet-B0**: Input size máº·c Ä‘á»‹nh 224x224, nhÆ°ng 128x128 váº«n hoáº¡t Ä‘á»™ng tá»‘t

**6. Grayscale â†’ RGB**
```python
transforms.Grayscale(num_output_channels=3)
```

**Táº¡i sao convert grayscale â†’ RGB?**
- EfficientNet-B0 pretrained trÃªn ImageNet (RGB 3 channels)
- Input pháº£i cÃ³ 3 channels Ä‘á»ƒ sá»­ dá»¥ng pretrained weights
- Copy grayscale vÃ o 3 channels: R=G=B

**7. Normalization**
```python
transforms.Normalize(
    mean=[0.485, 0.456, 0.406],  # ImageNet mean
    std=[0.229, 0.224, 0.225]    # ImageNet std
)
```

**Táº¡i sao normalize?**
- Chuáº©n hÃ³a pixel values vá» range [-1, 1]
- Model pretrained Ä‘Ã£ quen vá»›i distribution nÃ y
- GiÃºp training á»•n Ä‘á»‹nh hÆ¡n

**CÃ´ng thá»©c:**
```
normalized = (pixel - mean) / std
```

#### 3.3. Augmentation Strategy

**Training:**
- Ãp dá»¥ng Táº¤T Cáº¢ augmentations
- Má»—i epoch, má»—i áº£nh Ä‘Æ°á»£c augment khÃ¡c nhau
- TÄƒng diversity tá»‘i Ä‘a

**Validation:**
- CHá»ˆ resize vÃ  normalize
- KhÃ´ng augment Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ chÃ­nh xÃ¡c

**Test Time Augmentation (TTA):**
- Ãp dá»¥ng cho shapes (class_id >= 10)
- Rotations: Â±5Â°, Â±10Â°
- Average probabilities tá»« cÃ¡c augmentations
- TÄƒng accuracy inference

---

### 4. CHI TIáº¾T Vá»€ CRAFT DETECTOR

#### 4.1. CRAFT lÃ  gÃ¬?

**CRAFT (Character Region Awareness For Text detection):**
- Deep learning model Ä‘á»ƒ detect text/characters trong áº£nh
- Pre-trained trÃªn MLT dataset (25k áº£nh Ä‘a ngÃ´n ngá»¯)
- Tá»‘t cho: Scene text, rotated text, complex backgrounds

#### 4.2. CÃ¡ch hoáº¡t Ä‘á»™ng

**Architecture:**
- **Backbone**: VGG16 (feature extractor)
- **Output**: 2 heatmaps
  - **Text Region Map**: VÃ¹ng cÃ³ text
  - **Character Link Map**: Káº¿t ná»‘i giá»¯a cÃ¡c characters

**Quy trÃ¬nh:**
```
1. Input image â†’ Resize (giá»¯ aspect ratio, max 1280px)
2. Forward pass qua CRAFT network
3. Output: 2 heatmaps (text regions + character links)
4. Post-processing: TÃ¬m bounding boxes tá»« heatmaps
5. Adjust coordinates vá» kÃ­ch thÆ°á»›c gá»‘c
```

**Thresholds:**
- **text_threshold=0.7**: Confidence Ä‘á»ƒ xÃ¡c Ä‘á»‹nh vÃ¹ng cÃ³ text
- **link_threshold=0.4**: Confidence Ä‘á»ƒ káº¿t ná»‘i characters
- **low_text=0.4**: Threshold tháº¥p Ä‘á»ƒ detect text má»

#### 4.3. Táº¡i sao dÃ¹ng CRAFT cho digits?

**Æ¯u Ä‘iá»ƒm:**
- Tá»‘t vá»›i **rotated text** (chá»¯ sá»‘ xoay)
- Tá»‘t vá»›i **complex backgrounds** (ná»n phá»©c táº¡p)
- Detect Ä‘Æ°á»£c **small characters** (chá»¯ sá»‘ nhá»)

**NhÆ°á»£c Ä‘iá»ƒm:**
- Cháº­m hÆ¡n Traditional CV (100-200ms vs 50-100ms)
- Cáº§n GPU Ä‘á»ƒ cháº¡y nhanh
- Model weights lá»›n (~85MB)

---

### 5. CHI TIáº¾T Vá»€ HYBRID DETECTOR

#### 5.1. Chiáº¿n lÆ°á»£c Hybrid

**Váº¥n Ä‘á»:**
- Traditional CV: Tá»‘t cho shapes, nhÆ°ng kÃ©m vá»›i digits nhá»/xoay
- CRAFT: Tá»‘t cho digits, nhÆ°ng khÃ´ng detect shapes tá»‘t

**Giáº£i phÃ¡p: Hybrid**
- CRAFT detect digits/text
- Traditional CV detect shapes (sau khi mask out text regions)
- Merge vÃ  deduplicate

#### 5.2. Quy trÃ¬nh chi tiáº¿t

**BÆ°á»›c 1: CRAFT detect text/digits**
```python
text_bboxes = self.craft_detector.detect(image)
```

**BÆ°á»›c 2: Mask out text regions**
```python
masked_image = self._mask_regions(image, text_bboxes)
# Váº½ white rectangles lÃªn cÃ¡c vÃ¹ng text
```

**Táº¡i sao mask?**
- TrÃ¡nh Traditional CV detect láº¡i digits (Ä‘Ã£ cÃ³ tá»« CRAFT)
- Chá»‰ Ä‘á»ƒ láº¡i vÃ¹ng shapes cho Traditional CV

**BÆ°á»›c 3: Traditional CV detect shapes**
```python
shape_bboxes = self.cv_detector.detect(masked_image)
```

**BÆ°á»›c 4: Merge vÃ  NMS**
```python
all_bboxes = self._merge_bboxes(text_bboxes, shape_bboxes)
```

#### 5.3. Non-Maximum Suppression (NMS)

**Má»¥c Ä‘Ã­ch:**
- Loáº¡i bá» overlapping boxes
- Giá»¯ box tá»‘t nháº¥t (thÆ°á»ng lÃ  box lá»›n hÆ¡n)

**IoU (Intersection over Union):**
```
IoU = (Intersection Area) / (Union Area)
```

**VÃ­ dá»¥:**
```
Box 1: (100, 100, 200, 200)  # area = 40000
Box 2: (150, 150, 200, 200)  # area = 40000
Intersection: (150, 150, 200, 200)  # area = 2500
Union: (100, 100, 250, 250)  # area = 22500
IoU = 2500 / 22500 = 0.11
```

**NMS Algorithm:**
1. Sort boxes theo area (lá»›n â†’ nhá»)
2. Vá»›i má»—i box:
   - TÃ­nh IoU vá»›i cÃ¡c box cÃ²n láº¡i
   - Náº¿u IoU > threshold â†’ loáº¡i bá» box nhá» hÆ¡n
   - Náº¿u box bá»‹ contain hoÃ n toÃ n â†’ loáº¡i bá»

**IoU Threshold:**
- **0.5**: Loáº¡i bá» boxes overlap >50%
- **0.2**: Loáº¡i bá» nhiá»u hÆ¡n (cho CRAFT - nhiá»u overlapping boxes)

---

### 6. CHI TIáº¾T Vá»€ TRAINING PROCESS

#### 6.1. Loss Function: CrossEntropyLoss

**CÃ´ng thá»©c:**
```
Loss = -log(P(correct_class))
```

**VÃ­ dá»¥:**
```
Predicted probabilities: [0.1, 0.8, 0.05, 0.05]  # 4 classes
True label: 1 (class thá»© 2)
Loss = -log(0.8) = 0.223
```

**Táº¡i sao dÃ¹ng CrossEntropy?**
- PhÃ¹ há»£p vá»›i multi-class classification
- Penalize máº¡nh khi predict sai
- Stable vÃ  converge nhanh

#### 6.2. Optimizer: Adam

**Adam (Adaptive Moment Estimation):**
- Káº¿t há»£p **Momentum** (tá»‘c Ä‘á»™) vÃ  **RMSprop** (adaptive learning rate)
- Tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh learning rate cho tá»«ng parameter

**Æ¯u Ä‘iá»ƒm:**
- Converge nhanh hÆ¡n SGD
- KhÃ´ng cáº§n tune learning rate nhiá»u
- PhÃ¹ há»£p vá»›i sparse gradients

**Learning Rate: 1e-4**
- Nhá» Ä‘á»ƒ fine-tune pretrained weights
- KhÃ´ng phÃ¡ vá»¡ features Ä‘Ã£ há»c

#### 6.3. Scheduler: ReduceLROnPlateau

**CÃ¡ch hoáº¡t Ä‘á»™ng:**
- Monitor validation accuracy
- Náº¿u accuracy khÃ´ng tÄƒng trong 2 epochs (patience=2)
- Giáº£m learning rate xuá»‘ng 50% (factor=0.5)

**Táº¡i sao?**
- Khi accuracy plateau â†’ cÃ³ thá»ƒ Ä‘ang á»Ÿ local minimum
- Giáº£m LR giÃºp tÃ¬m Ä‘Æ°á»£c minimum tá»‘t hÆ¡n
- Fine-tuning tá»‘t hÆ¡n

#### 6.4. Batch Size: 64

**Táº¡i sao 64?**
- **QuÃ¡ nhá» (<32)**: Gradient khÃ´ng á»•n Ä‘á»‹nh, training cháº­m
- **QuÃ¡ lá»›n (>128)**: Tá»‘n memory, cÃ³ thá»ƒ khÃ´ng fit vÃ o GPU
- **64**: CÃ¢n báº±ng tá»‘t giá»¯a stability vÃ  speed

**Memory calculation:**
```
Batch size 64, Image 128x128x3
Memory per image: 128 * 128 * 3 * 4 bytes = 196KB
Memory per batch: 196KB * 64 = 12.5MB
+ Model weights: ~20MB
+ Gradients: ~20MB
Total: ~52.5MB (fit vÃ o GPU 6GB+)
```

#### 6.5. Epochs: 20

**Táº¡i sao 20?**
- Vá»›i pretrained model, chá»‰ cáº§n vÃ i epochs Ä‘á»ƒ fine-tune
- Sau epoch 5-10, accuracy Ä‘Ã£ Ä‘áº¡t ~98%
- 20 epochs Ä‘áº£m báº£o convergence

**Early Stopping:**
- LÆ°u model tá»‘t nháº¥t (best validation accuracy)
- TrÃ¡nh overfitting

---

### 7. CHI TIáº¾T Vá»€ POST-PROCESSING

#### 7.1. Target Filtering

**Má»¥c Ä‘Ã­ch:**
- Cho phÃ©p user chá»n chá»‰ detect digits, chá»‰ shapes, hoáº·c cáº£ hai

**Implementation:**
```python
if target_classes == 'digits':
    return class_id in [0, 1, 2, ..., 9]
elif target_classes == 'shapes':
    return class_id in [10, 11, ..., 18]
else:  # 'all'
    return True
```

#### 7.2. Reading Order Sorting

**Má»¥c Ä‘Ã­ch:**
- Sáº¯p xáº¿p detections theo thá»© tá»± Ä‘á»c tá»± nhiÃªn (top-to-bottom, left-to-right)

**Algorithm:**
1. TÃ­nh y_center cá»§a má»—i box
2. Group boxes vÃ o rows (tolerance = 50% avg height)
3. Sort rows theo y (top â†’ bottom)
4. Sort boxes trong má»—i row theo x (left â†’ right)

**VÃ­ dá»¥:**
```
Input boxes: [(100, 200), (50, 100), (300, 150), (200, 100)]
After sorting: [(50, 100), (200, 100), (300, 150), (100, 200)]
              Row 1      Row 1      Row 2      Row 3
```

#### 7.3. Test Time Augmentation (TTA)

**Má»¥c Ä‘Ã­ch:**
- TÄƒng accuracy inference báº±ng cÃ¡ch average predictions tá»« nhiá»u augmentations

**Chá»‰ Ã¡p dá»¥ng cho shapes:**
- Digits: KhÃ´ng cáº§n (Ä‘Ã£ Ä‘á»§ chÃ­nh xÃ¡c)
- Shapes: Cáº§n TTA Ä‘á»ƒ phÃ¢n biá»‡t tá»‘t hÆ¡n (Ä‘áº·c biá»‡t Nonagon/Octagon/Circle)

**Quy trÃ¬nh:**
```python
if predicted_class >= 10:  # Shape
    # Original prediction
    probs_original = model(crop)
    
    # Rotate +5Â°
    crop_rot5 = rotate(crop, 5)
    probs_rot5 = model(crop_rot5)
    
    # Rotate -5Â°
    crop_rot_neg5 = rotate(crop, -5)
    probs_rot_neg5 = model(crop_rot_neg5)
    
    # Average
    final_probs = (probs_original + probs_rot5 + probs_rot_neg5) / 3
```

**Káº¿t quáº£:**
- Accuracy tÄƒng ~0.5-1% cho shapes
- Trade-off: Inference cháº­m hÆ¡n 3-5x

---

### 8. CÃC CÃ‚U Há»I THÆ¯á»œNG Gáº¶P

#### Q1: Táº¡i sao khÃ´ng dÃ¹ng YOLO/SSD cho detection?

**Tráº£ lá»i:**
- YOLO/SSD cáº§n train riÃªng trÃªn dataset cÃ³ labels (bounding boxes)
- Dataset hiá»‡n táº¡i chá»‰ cÃ³ class labels, khÃ´ng cÃ³ bounding box labels
- Traditional CV + CRAFT khÃ´ng cáº§n training, hoáº¡t Ä‘á»™ng out-of-the-box
- Äá»§ tá»‘t cho use case nÃ y (digits/shapes trÃªn ná»n sÃ¡ng)

#### Q2: Táº¡i sao khÃ´ng dÃ¹ng ResNet thay vÃ¬ EfficientNet?

**Tráº£ lá»i:**
- EfficientNet tá»‘i Æ°u hÆ¡n vá» accuracy/efficiency trade-off
- Vá»›i cÃ¹ng accuracy, EfficientNet nhá» hÆ¡n vÃ  nhanh hÆ¡n ResNet
- EfficientNet-B0: ~4M parameters vs ResNet-18: ~11M parameters

#### Q3: Táº¡i sao input size 128x128 thay vÃ¬ 224x224 (ImageNet standard)?

**Tráº£ lá»i:**
- 128x128 Ä‘á»§ Ä‘á»ƒ nháº­n diá»‡n digits/shapes (objects Ä‘Æ¡n giáº£n)
- Nhá» hÆ¡n â†’ nhanh hÆ¡n, Ã­t memory hÆ¡n
- Trade-off: CÃ³ thá»ƒ máº¥t má»™t chÃºt accuracy, nhÆ°ng váº«n Ä‘áº¡t 99%

#### Q4: Táº¡i sao balanced sampling 67% shapes?

**Tráº£ lá»i:**
- MNIST: 60K images
- Shapes: 90K images (nhÆ°ng chá»‰ sample 67% = ~60K)
- Balance dataset Ä‘á»ƒ model khÃ´ng bias vá» má»™t class nÃ o
- Náº¿u khÃ´ng balance: Model cÃ³ thá»ƒ há»c tá»‘t digits nhÆ°ng kÃ©m shapes (hoáº·c ngÆ°á»£c láº¡i)

#### Q5: Táº¡i sao dÃ¹ng Grayscale â†’ RGB thay vÃ¬ train model má»›i cho grayscale?

**Tráº£ lá»i:**
- Sá»­ dá»¥ng pretrained weights (Ä‘Ã£ train trÃªn RGB)
- Náº¿u train model má»›i cho grayscale â†’ máº¥t lá»£i Ã­ch cá»§a pretrained weights
- Grayscale â†’ RGB Ä‘Æ¡n giáº£n vÃ  hiá»‡u quáº£ hÆ¡n

---

### 9. Tá»I Æ¯U HÃ“A VÃ€ Cáº¢I TIáº¾N

#### 9.1. Táº¡i sao tÄƒng input size tá»« 64 â†’ 128?

**Váº¥n Ä‘á» vá»›i 64x64:**
- KhÃ³ phÃ¢n biá»‡t Nonagon (9 cáº¡nh) vÃ  Circle
- KhÃ³ phÃ¢n biá»‡t Octagon (8 cáº¡nh) vÃ  Circle
- Edges bá»‹ má» khi resize nhá»

**Giáº£i phÃ¡p: 128x128**
- Giá»¯ Ä‘Æ°á»£c nhiá»u chi tiáº¿t hÆ¡n
- Accuracy tÄƒng: Circle 76% â†’ 90%+, Nonagon 73% â†’ 85%+

#### 9.2. Táº¡i sao augmentation máº¡nh hÆ¡n?

**Rotation: 15Â° â†’ 30Â°**
- TÄƒng diversity
- Model há»c Ä‘Æ°á»£c vá»›i gÃ³c xoay lá»›n hÆ¡n

**ThÃªm Perspective:**
- MÃ´ phá»ng gÃ³c chá»¥p thá»±c táº¿
- TÄƒng robustness

**ThÃªm ColorJitter:**
- MÃ´ phá»ng Ä‘iá»u kiá»‡n Ã¡nh sÃ¡ng khÃ¡c nhau
- TÄƒng generalization

---

### 10. METRICS VÃ€ ÄÃNH GIÃ

#### 10.1. Accuracy Metrics

**Overall Accuracy:**
```
Accuracy = (Correct Predictions) / (Total Predictions)
```

**Per-Class Accuracy:**
```
Class Accuracy = (Correct for Class) / (Total for Class)
```

**Confusion Matrix:**
- Ma tráº­n NxN (N=19 classes)
- HÃ ng i, cá»™t j: Sá»‘ lÆ°á»£ng class i bá»‹ predict thÃ nh class j
- ÄÆ°á»ng chÃ©o: Correct predictions
- Off-diagonal: Misclassifications

#### 10.2. Táº¡i sao Nonagon khÃ³ nháº¥t?

**LÃ½ do:**
- Nonagon (9 cáº¡nh) ráº¥t giá»‘ng Circle khi nhÃ¬n tá»« xa hoáº·c khi resolution tháº¥p
- Chá»‰ khÃ¡c nhau á»Ÿ sá»‘ cáº¡nh (9 vs vÃ´ sá»‘)
- Model dá»… nháº§m â†’ accuracy tháº¥p nháº¥t (94.69%)

**Giáº£i phÃ¡p:**
- TÄƒng input size (64â†’128)
- TTA vá»›i rotations
- Váº«n cÃ²n room for improvement

---

**Káº¿t luáº­n:**
README nÃ y Ä‘Ã£ giáº£i thÃ­ch chi tiáº¿t vá»:
- âœ… Pretrained models vÃ  Transfer Learning
- âœ… Contour detection vÃ  bounding boxes
- âœ… Data augmentation chi tiáº¿t
- âœ… CRAFT detector
- âœ… Hybrid detector
- âœ… Training process
- âœ… Post-processing
- âœ… CÃ¡c cÃ¢u há»i thÆ°á»ng gáº·p

Báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng cÃ¡c pháº§n nÃ y Ä‘á»ƒ tráº£ lá»i cÃ¡c cÃ¢u há»i sÃ¢u tá»« tháº§y giÃ¡o!

## ğŸ“Š Dataset

### MNIST Digits
- **Sá»‘ lÆ°á»£ng**: 60,000 training images
- **Format**: 28x28 grayscale
- **Classes**: 10 (0-9)
- **Source**: MNIST Competition dataset

### Shapes Dataset
- **Sá»‘ lÆ°á»£ng**: ~90,000 images
- **Format**: Various sizes, grayscale
- **Classes**: 9 (Circle, Triangle, Square, Pentagon, Hexagon, Heptagon, Octagon, Nonagon, Star)
- **Generation**: Synthetic vá»›i random transformations

### Training Strategy
- **Balanced Sampling**: 67% shapes Ä‘á»ƒ balance vá»›i MNIST
- **Train/Val Split**: 85/15 vá»›i stratification
- **Total Training**: ~100,000 images
- **Total Validation**: ~18,000 images

## ğŸ¯ TÃ­nh nÄƒng ná»•i báº­t

### 1. Unified Classification
- âœ… Má»™t model duy nháº¥t cho 19 classes
- âœ… KhÃ´ng cáº§n separate models cho digits vÃ  shapes
- âœ… Dá»… maintain vÃ  deploy

### 2. Flexible Detection
- âœ… **Traditional CV**: Nhanh, tá»‘t cho shapes
- âœ… **CRAFT**: Tá»‘t cho digits vÃ  text
- âœ… **Hybrid**: Tá»‘i Æ°u cho cáº£ hai

### 3. Target Filtering
- âœ… Chá»‰ detect digits: `--target digits`
- âœ… Chá»‰ detect shapes: `--target shapes`
- âœ… Detect cáº£ hai: `--target all`

### 4. Synthetic Data Generation
- âœ… Tá»± Ä‘á»™ng táº¡o test images
- âœ… Control sá»‘ lÆ°á»£ng digits vÃ  shapes
- âœ… KhÃ´ng overlap giá»¯a cÃ¡c objects
- âœ… Ground truth labels

### 5. MQTT Real-time Processing
- âœ… Nháº­n áº£nh tá»« frontend qua MQTT
- âœ… Xá»­ lÃ½ vÃ  tráº£ káº¿t quáº£ real-time
- âœ… Base64 encoding cho images
- âœ… JSON format cho results

### 6. Reading Order Sorting
- âœ… Sort detections theo thá»© tá»± Ä‘á»c tá»± nhiÃªn
- âœ… Top-to-bottom, left-to-right
- âœ… Group objects vÃ o rows

## ğŸ“ˆ Káº¿t quáº£ vÃ  Performance

### Classification Accuracy

| Category | Training | Validation | Notes |
|----------|----------|------------|-------|
| **Overall** | 99.3% | **99.14%** | 19 classes combined |
| **Digits (0-9)** | 99.5% | 99.3% | High accuracy |
| **Shapes** | 99.0% | 98.5% | Good, some confusion Circle/Nonagon |
| **Best Class** | - | 99.90% | Digit "1", Triangle |
| **Worst Class** | - | 94.69% | Nonagon (confused with Circle) |

### Per-Class Performance

**Top Performers:**
- Digit "1": 99.90%
- Digit "8": 99.89%
- Triangle: 99.90%
- Star: 99.70%

**Challenging Classes:**
- Nonagon: 94.69% (confused with Circle ~4%)
- Octagon: 97.96% (confused with Circle ~0.78%)

### Inference Speed

| Component | Time (ms) | Notes |
|-----------|-----------|-------|
| **Detection (Traditional)** | 50-100 | Fast, CPU-friendly |
| **Detection (CRAFT)** | 100-200 | Slower, requires GPU |
| **Detection (Hybrid)** | 150-250 | Combines both |
| **Classification (per object)** | 5-10 | EfficientNet-B0 |
| **Total (5 objects)** | 100-300 | End-to-end |

*Tested on RTX 4050 Laptop GPU*

### Model Size

- **EfficientNet-B0**: ~5.3M parameters
- **Model weights**: ~20MB (.pth file)
- **CRAFT weights**: ~85MB
- **Total**: ~105MB

## ğŸ”„ Flow hoáº¡t Ä‘á»™ng chi tiáº¿t

### 1. Training Flow

```
Load Datasets (MNIST + Shapes)
    â†“
Create Label Mapping (0-18)
    â†“
Split Train/Val (85/15)
    â†“
Apply Augmentation
    â†“
Train EfficientNet-B0
    â†“
Validate & Save Best Model
    â†“
Evaluate Performance
```

### 2. Inference Flow

```
Input Image
    â†“
Preprocessing (Denoise, CLAHE, etc.)
    â†“
Detection (Traditional/CRAFT/Hybrid)
    â†“
Crop Bounding Boxes
    â†“
Resize to 128x128
    â†“
Classification (EfficientNet-B0)
    â†“
Filter by Target Classes
    â†“
Sort by Reading Order
    â†“
Visualize & Output JSON
```

### 3. MQTT Flow

```
Frontend â†’ image/create (numberDigit, numberShape)
    â†“
AI: Generate Synthetic Image
    â†“
AI â†’ image/input/create (image base64 + count)
    â†“
Frontend: Display Image
    â†“
User: Click "Process"
    â†“
Frontend â†’ image/input (image base64 + label + count)
    â†“
AI: Detect & Classify (Auto Hybrid if count exists)
    â†“
AI â†’ image/output (image base64 + detections JSON)
    â†“
Frontend: Display Results
```

## ğŸ“– MÃ´ táº£ chi tiáº¿t

Dá»± Ã¡n nÃ y xÃ¢y dá»±ng má»™t há»‡ thá»‘ng hoÃ n chá»‰nh Ä‘á»ƒ nháº­n diá»‡n vÃ  phÃ¢n loáº¡i:
- **10 chá»¯ sá»‘**: 0-9 (tá»« MNIST dataset)
- **9 hÃ¬nh há»c**: Circle, Triangle, Square, Pentagon, Hexagon, Heptagon, Octagon, Nonagon, Star

### Pipeline

```
Input Image â†’ Detection (Traditional CV/CRAFT) â†’ Classification (EfficientNet-B0) â†’ Output (Annotated Image + JSON)
```

### Äáº·c Ä‘iá»ƒm ná»•i báº­t

- âœ… **19 classes**: Digits (0-9) + Shapes (9 loáº¡i)
- âœ… **Äá»™ chÃ­nh xÃ¡c cao**: ~99% validation accuracy
- âœ… **Inference nhanh**: ~100-300ms/áº£nh
- âœ… **Linh hoáº¡t**: Há»— trá»£ nhiá»u phÆ°Æ¡ng phÃ¡p detection
- âœ… **Dá»… sá»­ dá»¥ng**: API Ä‘Æ¡n giáº£n vÃ  rÃµ rÃ ng
- âœ… **MQTT Integration**: Real-time processing vá»›i frontend
- âœ… **Synthetic Data Generation**: Tá»± Ä‘á»™ng táº¡o test images

## ğŸš€ CÃ i Ä‘áº·t

### YÃªu cáº§u há»‡ thá»‘ng

- Python 3.8+
- CUDA 11.8+ (optional, cho GPU acceleration)
- RAM: 8GB+ (16GB recommended)
- Disk space: ~5GB (cho datasets vÃ  models)

### BÆ°á»›c 1: Clone repository

```bash
# Clone project tá»« GitHub
git clone https://github.com/your-username/BTL_XLA.git
cd BTL_XLA
```

### BÆ°á»›c 2: Setup Conda Environment (Khuyáº¿n nghá»‹)

```bash
# Táº¡o conda environment má»›i
conda create -n btl_xla python=3.10 -y

# Activate environment
conda activate btl_xla

# CÃ i Ä‘áº·t PyTorch vá»›i CUDA (náº¿u cÃ³ GPU)
conda install pytorch torchvision pytorch-cuda=11.8 -c pytorch -c nvidia -y

# CÃ i Ä‘áº·t cÃ¡c dependencies cÃ²n láº¡i
pip install -r requirements.txt
```

**Hoáº·c náº¿u chá»‰ dÃ¹ng CPU:**

```bash
conda create -n btl_xla python=3.10 -y
conda activate btl_xla
conda install pytorch torchvision cpuonly -c pytorch -y
pip install -r requirements.txt
```

### BÆ°á»›c 3: Táº£i vÃ  chuáº©n bá»‹ dá»¯ liá»‡u

#### 3.1. MNIST Dataset

```bash
# Giáº£i nÃ©n mnist_competition.zip (náº¿u cÃ³)
unzip mnist_competition.zip

# Hoáº·c táº£i tá»« Kaggle/Google Drive
# Cáº¥u trÃºc: mnist_competition/train/ vÃ  mnist_competition/train_label.csv
```

#### 3.2. Shapes Dataset

```bash
# Giáº£i nÃ©n dataset trong Shapes_Classifier
cd Shapes_Classifier
unzip dataset.zip
cd ..
```

#### 3.3. CRAFT Weights (cho Hybrid Detector)

```bash
# Táº¡o thÆ° má»¥c weights
mkdir weights

# Táº£i CRAFT weights
wget https://drive.google.com/uc?id=1Jk4eGD7crsqCCg9C9VjCLkMN3ze8kutZ -O weights/craft_mlt_25k.pth

# Hoáº·c dÃ¹ng gdown
pip install gdown
gdown https://drive.google.com/uc?id=1Jk4eGD7crsqCCg9C9VjCLkMN3ze8kutZ -O weights/craft_mlt_25k.pth
```

### Chuáº©n bá»‹ dá»¯ liá»‡u hoÃ n táº¥t

Äáº£m báº£o cáº¥u trÃºc thÆ° má»¥c nhÆ° sau:

```
BTL_XLA/
â”œâ”€â”€ mnist_competition/
â”‚   â”œâ”€â”€ train/              # 60,000 MNIST images
â”‚   â”œâ”€â”€ train_label.csv
â”‚   â””â”€â”€ public_test/
â”œâ”€â”€ Shapes_Classifier/
â”‚   â””â”€â”€ dataset/output/     # 90,000 shape images (Circle, Square, etc.)
â”œâ”€â”€ weights/
â”‚   â””â”€â”€ craft_mlt_25k.pth   # CRAFT pretrained weights (~85MB)
â”œâ”€â”€ unified_model_19classes_best.pth  # Trained classifier
â””â”€â”€ label_mapping.json
```

## ğŸ“š HÆ°á»›ng dáº«n sá»­ dá»¥ng

### 1. Training Classifier

Train mÃ´ hÃ¬nh EfficientNet-B0 trÃªn 19 classes:

#### Sá»­ dá»¥ng Python Script

```bash
python train_unified_classifier.py --epochs 20 --batch-size 64
```

**Tham sá»‘:**
- `--epochs`: Sá»‘ epoch (máº·c Ä‘á»‹nh: 20)
- `--batch-size`: Batch size (máº·c Ä‘á»‹nh: 64)
- `--lr`: Learning rate (máº·c Ä‘á»‹nh: 1e-4)
- `--device`: 'cuda' hoáº·c 'cpu'

#### Sá»­ dá»¥ng Jupyter Notebook

```bash
jupyter notebook train_unified_classifier.ipynb
```

**Output:**
- `unified_model_19classes_best.pth`: Model Ä‘Ã£ train
- `label_mapping.json`: Ãnh xáº¡ class labels
- `training_history.png`: Biá»ƒu Ä‘á»“ loss/accuracy

### 2. Pipeline - Inference trÃªn áº£nh

#### 2.1. Xá»­ lÃ½ áº£nh cÃ³ sáºµn (táº¥t cáº£ classes)

```bash
python pipeline.py --image Sample.png --output Sample_result.png
```

#### 2.2. Chá»‰ nháº­n diá»‡n SHAPES

```bash
python pipeline.py --image Sample.png --target shapes --output Sample_shapes_only.png
```

#### 2.3. Chá»‰ nháº­n diá»‡n DIGITS

```bash
python pipeline.py --image Sample.png --target digits --output Sample_digits_only.png
```

#### 2.4. Sá»­ dá»¥ng Hybrid Detector (CRAFT + Traditional CV)

```bash
python pipeline.py --image Sample.png --detector hybrid --target all
```

#### 2.5. Táº¡o áº£nh test synthetic tá»± Ä‘á»™ng

```bash
# Táº¡o áº£nh vá»›i 5 objects (máº·c Ä‘á»‹nh)
python pipeline.py --generate

# Táº¡o áº£nh vá»›i 10 objects
python pipeline.py --generate --num-objects 10

# Táº¡o vÃ  chá»‰ detect shapes
python pipeline.py --generate --num-objects 8 --target shapes
```

**Pipeline Output:**
- `*_result.png`: áº¢nh Ä‘Æ°á»£c annotate vá»›i bounding boxes
- `*_result.json`: Káº¿t quáº£ detection á»Ÿ Ä‘á»‹nh dáº¡ng JSON

**Pipeline Arguments:**

| Argument | Choices | Default | MÃ´ táº£ |
|----------|---------|---------|-------|
| `--image` | path | None | ÄÆ°á»ng dáº«n áº£nh input |
| `--output` | path | Auto | ÄÆ°á»ng dáº«n áº£nh output |
| `--target` | `digits`, `shapes`, `all` | `all` | Loáº¡i objects cáº§n detect |
| `--detector` | `traditional`, `hybrid` | `traditional` | PhÆ°Æ¡ng phÃ¡p detection |
| `--generate` | flag | False | Táº¡o áº£nh test synthetic |
| `--num-objects` | int | 5 | Sá»‘ objects trong synthetic scene |
| `--model` | path | `unified_model_19classes_best.pth` | Model weights |
| `--labels` | path | `label_mapping.json` | Label mapping |
| `--device` | `cuda`, `cpu` | Auto | Device Ä‘á»ƒ inference |

### 3. Evaluation

ÄÃ¡nh giÃ¡ hiá»‡u nÄƒng model:

```bash
python evaluate_model.py
```

**Output:**
- Per-class accuracy report
- Confusion matrix
- Classification report
- `per_class_performance.csv`

### 4. Sá»­ dá»¥ng nhÆ° má»™t module

```python
from pipeline import UnifiedPipeline

# Khá»Ÿi táº¡o pipeline - Detect ALL
pipeline = UnifiedPipeline(
    model_path='unified_model_19classes_best.pth',
    label_mapping_path='label_mapping.json',
    device='cuda',  # hoáº·c 'cpu'
    detector_type='traditional',  # hoáº·c 'hybrid'
    target_classes='all'  # 'digits', 'shapes', hoáº·c 'all'
)

# Xá»­ lÃ½ áº£nh
results = pipeline.process_file('test_image.png')

# Káº¿t quáº£
print(f"Detected {len(results['labels'])} objects")
for label, conf in zip(results['labels'], results['confidences']):
    print(f"Class: {label}, Confidence: {conf:.2%}")
```

#### Táº¡o synthetic data

```python
from pipeline import generate_synthetic_scene
import cv2

# Táº¡o scene vá»›i 10 random objects
canvas, ground_truth = generate_synthetic_scene(
    mnist_dir='mnist_competition/train',
    shapes_dir='Shapes_Classifier/dataset/output',
    mnist_csv='mnist_competition/train_label.csv',
    num_objects=10,
    canvas_size=(800, 600),
    seed=42
)

# LÆ°u áº£nh
cv2.imwrite('my_test_scene.png', canvas)

# In ground truth
print("Ground truth labels:", [item[4] for item in ground_truth])
```

## ğŸ“ Cáº¥u trÃºc dá»± Ã¡n

```
BTL_XLA/
â”œâ”€â”€ mnist_competition/              # MNIST dataset
â”‚   â”œâ”€â”€ train/                      # Training images
â”‚   â”œâ”€â”€ public_test/                # Test images
â”‚   â””â”€â”€ train_label.csv             # Labels
â”œâ”€â”€ Shapes_Classifier/              # Shapes dataset
â”‚   â””â”€â”€ dataset/output/             # Shape images
â”œâ”€â”€ train_unified_classifier.py     # Training script
â”œâ”€â”€ detect_objects.py               # Detection module
â”œâ”€â”€ pipeline.py                     # End-to-end pipeline
â”œâ”€â”€ preprocess_grid_image.py        # Preprocessing utilities
â”œâ”€â”€ unified_model_19classes_best.pth    # Trained model
â”œâ”€â”€ label_mapping.json              # Class mapping
â”œâ”€â”€ requirements.txt                # Dependencies
â”œâ”€â”€ CRAFT_SHAPES_GUIDE.md          # Detailed guide
â””â”€â”€ README.md                       # This file
```

## ğŸ¯ Class Mapping

| Class ID | Label | Category |
|----------|-------|----------|
| 0-9 | 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 | Digits |
| 10 | Circle | Shape |
| 11 | Heptagon | Shape |
| 12 | Hexagon | Shape |
| 13 | Nonagon | Shape |
| 14 | Octagon | Shape |
| 15 | Pentagon | Shape |
| 16 | Square | Shape |
| 17 | Star | Shape |
| 18 | Triangle | Shape |

## ğŸ“Š Hiá»‡u nÄƒng

### Classification Accuracy

| Dataset | Training | Validation |
|---------|----------|------------|
| MNIST Digits | 99.5% | 99.3% |
| Shapes | 99.0% | 98.5% |
| **Unified (19 classes)** | **99.3%** | **99.0%** |

### Inference Speed

| Component | Time (ms) |
|-----------|-----------|
| Detection | 50-150 |
| Classification | 5-10 per object |
| **Total** | **100-300** |

*Tested on RTX 4050*

## ğŸ”§ Advanced Usage

### Custom Detection Parameters

```python
from detect_objects import TraditionalDetector

detector = TraditionalDetector(
    min_area=200,
    max_area=30000,
    aspect_ratio_range=(0.2, 5.0)
)

bboxes = detector.detect(image)
```

### Synthetic Data Generation

```python
from pipeline import generate_synthetic_scene

canvas, ground_truth = generate_synthetic_scene(
    mnist_dir='mnist_competition/train',
    shapes_dir='Shapes_Classifier/dataset/output',
    mnist_csv='mnist_competition/train_label.csv',
    num_objects=5,
    canvas_size=(800, 600),
    seed=42
)
```

## ğŸ”„ HÆ°á»›ng dáº«n Push/Pull vá»›i GitHub (Sá»­ dá»¥ng Conda)

### Láº§n Ä‘áº§u push lÃªn GitHub

#### BÆ°á»›c 1: Táº¡o repository trÃªn GitHub

1. VÃ o [GitHub](https://github.com)
2. Click **New repository**
3. Äáº·t tÃªn: `BTL_XLA`
4. Chá»n **Public** hoáº·c **Private**
5. **KHÃ”NG** chá»n "Initialize with README"
6. Click **Create repository**

#### BÆ°á»›c 2: Setup Git local (náº¿u chÆ°a cÃ³)

```bash
# Kiá»ƒm tra Git Ä‘Ã£ cÃ i chÆ°a
git --version

# Náº¿u chÆ°a cÃ³, cÃ i Git
# Windows: Download tá»« https://git-scm.com/
# Linux: sudo apt install git
# macOS: brew install git

# Config thÃ´ng tin (chá»‰ cáº§n 1 láº§n)
git config --global user.name "Your Name"
git config --global user.email "your.email@example.com"
```

#### BÆ°á»›c 3: Khá»Ÿi táº¡o Git repository

```bash
# Activate conda environment
conda activate btl_xla

# Di chuyá»ƒn vÃ o thÆ° má»¥c project
cd D:\BTL_XLA

# Khá»Ÿi táº¡o Git repository
git init

# Kiá»ƒm tra status
git status
```

#### BÆ°á»›c 4: Add files vÃ  commit

```bash
# Add táº¥t cáº£ files (theo .gitignore)
git add .

# Kiá»ƒm tra nhá»¯ng gÃ¬ sáº½ commit
git status

# Commit láº§n Ä‘áº§u
git commit -m "Initial commit: Unified Digits & Shapes Recognition System"
```

#### BÆ°á»›c 5: Káº¿t ná»‘i vá»›i GitHub vÃ  push

```bash
# ThÃªm remote repository (thay YOUR_USERNAME báº±ng username GitHub cá»§a báº¡n)
git remote add origin https://github.com/YOUR_USERNAME/BTL_XLA.git

# Kiá»ƒm tra remote
git remote -v

# Push lÃªn GitHub (branch main)
git branch -M main
git push -u origin main
```

**LÆ°u Ã½ vá» viá»‡c push:**
- Theo `.gitignore`, nhá»¯ng thá»© SAU sáº½ Ä‘Æ°á»£c push:
  - âœ… `craft_repo/` (full folder)
  - âœ… `mnist_competition.zip` (file nÃ©n)
  - âœ… `mnist_competition/*.csv` (cÃ¡c file CSV)
  - âœ… `Shapes_Classifier/` (trá»« folder `dataset/`)
  - âœ… `weights/craft_mlt_25k.pth`
  - âœ… `unified_model_19classes_best.pth`
  - âœ… Táº¥t cáº£ `.py`, `.ipynb`, `.md`, `requirements.txt`
  - âœ… `Sample.png`, `label_mapping.json`

- Nhá»¯ng thá»© SAU sáº½ KHÃ”NG push (Ä‘Ã£ bá»‹ ignore):
  - âŒ `mnist_competition/train/` (60,000 áº£nh)
  - âŒ `mnist_competition/public_test/` (10,000 áº£nh)
  - âŒ `Shapes_Classifier/dataset/` (90,000 áº£nh)
  - âŒ `__pycache__/`, `.ipynb_checkpoints/`
  - âŒ `*_result.png`, `*_result.json`
  - âŒ `Test_*.png`, `Test_*.jpg`

### Khi muá»‘n update code (push thay Ä‘á»•i má»›i)

```bash
# Activate environment
conda activate btl_xla

# Kiá»ƒm tra thay Ä‘á»•i
git status

# Add files Ä‘Ã£ thay Ä‘á»•i
git add .

# Commit vá»›i message mÃ´ táº£
git commit -m "Update: Improved detection accuracy"

# Push lÃªn GitHub
git push origin main
```

### Khi muá»‘n táº£i code má»›i (pull tá»« GitHub)

```bash
# Pull code má»›i nháº¥t
git pull origin main

# Náº¿u bá»‹ conflict, Git sáº½ bÃ¡o - cáº§n resolve manually
```

### Clone project tá»« GitHub (cho mÃ¡y khÃ¡c)

```bash
# Clone repository
git clone https://github.com/YOUR_USERNAME/BTL_XLA.git
cd BTL_XLA

# Setup conda environment
conda create -n btl_xla python=3.10 -y
conda activate btl_xla

# CÃ i Ä‘áº·t dependencies
conda install pytorch torchvision pytorch-cuda=11.8 -c pytorch -c nvidia -y
pip install -r requirements.txt

# Giáº£i nÃ©n datasets
unzip mnist_competition.zip
cd Shapes_Classifier
unzip dataset.zip
cd ..

# Cháº¡y pipeline
python pipeline.py --generate --num-objects 5
```

### Git Commands thÆ°á»ng dÃ¹ng

```bash
# Xem lá»‹ch sá»­ commit
git log --oneline

# Xem thay Ä‘á»•i chÆ°a commit
git diff

# Há»§y thay Ä‘á»•i chÆ°a add
git restore filename.py

# Táº¡o branch má»›i
git checkout -b feature/new-feature

# Chuyá»ƒn branch
git checkout main

# Merge branch
git merge feature/new-feature

# Xem táº¥t cáº£ branches
git branch -a
```

## ğŸ› Troubleshooting

### Lá»—i: Model khÃ´ng load Ä‘Æ°á»£c

```bash
# Kiá»ƒm tra PyTorch version
python -c "import torch; print(torch.__version__)"

# Kiá»ƒm tra CUDA
python -c "import torch; print(torch.cuda.is_available())"
```

### Lá»—i: Out of memory

- Giáº£m batch size: `--batch-size 32`
- Sá»­ dá»¥ng CPU: `--device cpu`
- Giáº£m resolution cá»§a áº£nh input

### Detection rate tháº¥p

- Äiá»u chá»‰nh threshold: `min_area=100`
- Thá»­ detector khÃ¡c: `--detector hybrid`

## ğŸ“„ TÃ i liá»‡u tham kháº£o

- [CRAFT Paper](https://arxiv.org/abs/1904.01941) - Character Region Awareness For Text detection
- [EfficientNet Paper](https://arxiv.org/abs/1905.11946) - Efficient Convolutional Neural Networks
- [CRAFT GitHub](https://github.com/clovaai/CRAFT-pytorch) - Official CRAFT implementation
- [Detailed Guide](CRAFT_SHAPES_GUIDE.md) - HÆ°á»›ng dáº«n chi tiáº¿t

## ğŸ¤ ÄÃ³ng gÃ³p

Má»i Ä‘Ã³ng gÃ³p Ä‘á»u Ä‘Æ°á»£c hoan nghÃªnh! Vui lÃ²ng:

1. Fork repository
2. Táº¡o feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Má»Ÿ Pull Request

## ğŸ“ License

Dá»± Ã¡n nÃ y Ä‘Æ°á»£c phÃ¡t triá»ƒn cho má»¥c Ä‘Ã­ch giÃ¡o dá»¥c.

## ğŸ‘¥ TÃ¡c giáº£

- Äá»“ Ã¡n mÃ´n Xá»­ lÃ½ áº£nh (Image Processing)
- TrÆ°á»ng Äáº¡i há»c...

## ğŸ™ Acknowledgments

- MNIST Dataset
- Shapes Dataset
- CRAFT-pytorch
- EfficientNet

---

**â­ Náº¿u project há»¯u Ã­ch, Ä‘á»«ng quÃªn cho má»™t star nhÃ©!**

## ğŸ“ LiÃªn há»‡

Náº¿u cÃ³ cÃ¢u há»i hoáº·c váº¥n Ä‘á», vui lÃ²ng táº¡o [Issue](../../issues) trÃªn GitHub.

