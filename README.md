# ğŸ¯ Unified Digits & Shapes Recognition System

Há»‡ thá»‘ng nháº­n diá»‡n chá»¯ sá»‘ viáº¿t tay vÃ  hÃ¬nh há»c trong áº£nh sá»­ dá»¥ng Deep Learning vá»›i kiáº¿n trÃºc Detection + Classification.

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## ğŸ“– Giá»›i thiá»‡u dá»± Ã¡n

### Tá»•ng quan

Dá»± Ã¡n **Unified Digits & Shapes Recognition System** lÃ  má»™t há»‡ thá»‘ng nháº­n diá»‡n Ä‘á»‘i tÆ°á»£ng hoÃ n chá»‰nh, cÃ³ kháº£ nÄƒng phÃ¡t hiá»‡n vÃ  phÃ¢n loáº¡i Ä‘á»“ng thá»i **chá»¯ sá»‘ viáº¿t tay** (0-9) vÃ  **hÃ¬nh há»c** (9 loáº¡i) trong cÃ¹ng má»™t áº£nh. Há»‡ thá»‘ng sá»­ dá»¥ng kiáº¿n trÃºc **hai giai Ä‘oáº¡n** (Two-Stage): **Detection** Ä‘á»ƒ tÃ¬m vá»‹ trÃ­ cÃ¡c Ä‘á»‘i tÆ°á»£ng, sau Ä‘Ã³ **Classification** Ä‘á»ƒ nháº­n diá»‡n loáº¡i cá»§a tá»«ng Ä‘á»‘i tÆ°á»£ng.

### Má»¥c tiÃªu

- ğŸ¯ XÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh thá»‘ng nháº¥t cÃ³ thá»ƒ nháº­n diá»‡n cáº£ chá»¯ sá»‘ vÃ  hÃ¬nh há»c trong cÃ¹ng má»™t pipeline
- ğŸ¯ Äáº¡t Ä‘á»™ chÃ­nh xÃ¡c cao (>99%) trÃªn cáº£ hai loáº¡i Ä‘á»‘i tÆ°á»£ng
- ğŸ¯ Tá»‘i Æ°u tá»‘c Ä‘á»™ inference Ä‘á»ƒ cÃ³ thá»ƒ Ã¡p dá»¥ng trong thá»±c táº¿
- ğŸ¯ Há»— trá»£ nhiá»u phÆ°Æ¡ng phÃ¡p detection linh hoáº¡t (Traditional CV, CRAFT, Hybrid)
- ğŸ¯ TÃ­ch há»£p MQTT Ä‘á»ƒ xá»­ lÃ½ real-time tá»« frontend

### á»¨ng dá»¥ng thá»±c táº¿

- ğŸ“ **Nháº­n diá»‡n chá»¯ sá»‘ viáº¿t tay**: Äá»c sá»‘ tá»« biá»ƒu máº«u, hÃ³a Ä‘Æ¡n, chá»©ng tá»«
- ğŸ”· **PhÃ¢n loáº¡i hÃ¬nh há»c**: PhÃ¢n tÃ­ch hÃ¬nh dáº¡ng trong áº£nh ká»¹ thuáº­t, báº£n váº½
- ğŸ“ **GiÃ¡o dá»¥c**: Há»— trá»£ há»c sinh nháº­n diá»‡n sá»‘ vÃ  hÃ¬nh há»c
- ğŸ­ **Tá»± Ä‘á»™ng hÃ³a**: Xá»­ lÃ½ áº£nh trong dÃ¢y chuyá»n sáº£n xuáº¥t
- ğŸ“± **Mobile Apps**: TÃ­ch há»£p vÃ o á»©ng dá»¥ng di Ä‘á»™ng Ä‘á»ƒ nháº­n diá»‡n real-time

## ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

### Pipeline tá»•ng quan

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input Image â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Stage 1: Object Detection     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ Traditional CV Detector  â”‚  â”‚
â”‚   â”‚ (Contour-based)          â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ CRAFT Detector           â”‚  â”‚
â”‚   â”‚ (Text/Character)         â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ Hybrid Detector          â”‚  â”‚
â”‚   â”‚ (CV + CRAFT combined)    â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Bounding Boxes (x, y, w, h)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Stage 2: Classification       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ EfficientNet-B0          â”‚  â”‚
â”‚   â”‚ (19 classes)             â”‚  â”‚
â”‚   â”‚ - 10 digits (0-9)        â”‚  â”‚
â”‚   â”‚ - 9 shapes               â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Labels + Confidences
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Stage 3: Post-processing      â”‚
â”‚   - Filter by target (digits/  â”‚
â”‚     shapes/all)                 â”‚
â”‚   - Sort by reading order       â”‚
â”‚   - Visualize annotations       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Output:                        â”‚
â”‚   - Annotated Image (PNG)       â”‚
â”‚   - JSON Results                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### CÃ¡c thÃ nh pháº§n chÃ­nh

#### 1. **Detection Module** (`detect_objects.py`)

**Traditional CV Detector:**
- Sá»­ dá»¥ng OpenCV Ä‘á»ƒ tÃ¬m contours
- Preprocessing: Denoising, CLAHE, Illumination correction
- Adaptive thresholding Ä‘á»ƒ tÃ¡ch foreground/background
- Filter theo area, aspect ratio Ä‘á»ƒ loáº¡i bá» noise

**CRAFT Detector:**
- Deep learning model Ä‘á»ƒ detect text/characters
- Pre-trained trÃªn MLT dataset (25k images)
- Tá»‘t cho viá»‡c detect chá»¯ sá»‘ vÃ  kÃ½ tá»±

**Hybrid Detector:**
- Káº¿t há»£p Traditional CV + CRAFT
- CRAFT detect digits, Traditional CV detect shapes
- Merge vÃ  deduplicate káº¿t quáº£
- Tá»‘i Æ°u cho áº£nh cÃ³ cáº£ digits vÃ  shapes

#### 2. **Classification Module** (`train_unified_classifier.py`)

**Model Architecture:**
- **Backbone**: EfficientNet-B0 (pre-trained trÃªn ImageNet)
- **Input**: 128x128 RGB images (grayscale converted)
- **Output**: 19 classes (10 digits + 9 shapes)
- **Augmentation**: Rotation, Affine, Perspective, ColorJitter (balanced Ä‘á»ƒ giá»¯ shape edges)

**Training Process:**
- Dataset: ~100,000 images (MNIST + Shapes)
- Epochs: 20
- Optimizer: Adam (lr=1e-4)
- Loss: CrossEntropy
- Validation accuracy: ~99.14%

#### 3. **Pipeline Module** (`pipeline.py`)

**Chá»©c nÄƒng:**
- Káº¿t há»£p Detection + Classification
- Filter theo target classes (digits/shapes/all)
- Sort detections theo reading order (top-to-bottom, left-to-right)
- Visualize vá»›i bounding boxes vÃ  labels
- Generate synthetic test images
- MQTT integration cho real-time processing

#### 4. **MQTT Integration**

**Topics:**
- `image/create`: Request generate áº£nh synthetic
- `image/input/create`: Response vá»›i áº£nh Ä‘Ã£ generate
- `image/input`: Request xá»­ lÃ½ áº£nh
- `image/output`: Response vá»›i káº¿t quáº£ detection

**Flow:**
```
Frontend â†’ image/create â†’ AI generate â†’ image/input/create â†’ Frontend
Frontend â†’ image/input â†’ AI process â†’ image/output â†’ Frontend
```

## ğŸ”¬ CÃ´ng nghá»‡ vÃ  phÆ°Æ¡ng phÃ¡p

### Deep Learning

- **EfficientNet-B0**: CNN architecture tá»‘i Æ°u vá» accuracy/efficiency
- **Transfer Learning**: Pre-trained trÃªn ImageNet, fine-tune trÃªn custom dataset
- **Data Augmentation**: TÄƒng diversity cá»§a training data

### Computer Vision

- **Contour Detection**: TÃ¬m boundaries cá»§a objects
- **Adaptive Thresholding**: Tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh threshold theo local regions
- **CLAHE**: Contrast Limited Adaptive Histogram Equalization
- **Morphological Operations**: LÃ m sáº¡ch vÃ  tÃ¡ch objects

### Text Detection

- **CRAFT**: Character Region Awareness For Text detection
- **Region Proposal**: TÃ¬m regions cÃ³ kháº£ nÄƒng chá»©a text
- **Link Prediction**: Káº¿t ná»‘i cÃ¡c characters thÃ nh words

### Preprocessing

- **Denoising**: Loáº¡i bá» noise trong áº£nh
- **Contrast Enhancement**: TÄƒng Ä‘á»™ tÆ°Æ¡ng pháº£n
- **Illumination Correction**: Chuáº©n hÃ³a Ã¡nh sÃ¡ng
- **Normalization**: Chuáº©n hÃ³a pixel values

## ğŸ“Š Dataset

### MNIST Digits
- **Sá»‘ lÆ°á»£ng**: 60,000 training images
- **Format**: 28x28 grayscale
- **Classes**: 10 (0-9)
- **Source**: MNIST Competition dataset

### Shapes Dataset
- **Sá»‘ lÆ°á»£ng**: ~90,000 images
- **Format**: Various sizes, grayscale
- **Classes**: 9 (Circle, Triangle, Square, Pentagon, Hexagon, Heptagon, Octagon, Nonagon, Star)
- **Generation**: Synthetic vá»›i random transformations

### Training Strategy
- **Balanced Sampling**: 67% shapes Ä‘á»ƒ balance vá»›i MNIST
- **Train/Val Split**: 85/15 vá»›i stratification
- **Total Training**: ~100,000 images
- **Total Validation**: ~18,000 images

## ğŸ¯ TÃ­nh nÄƒng ná»•i báº­t

### 1. Unified Classification
- âœ… Má»™t model duy nháº¥t cho 19 classes
- âœ… KhÃ´ng cáº§n separate models cho digits vÃ  shapes
- âœ… Dá»… maintain vÃ  deploy

### 2. Flexible Detection
- âœ… **Traditional CV**: Nhanh, tá»‘t cho shapes
- âœ… **CRAFT**: Tá»‘t cho digits vÃ  text
- âœ… **Hybrid**: Tá»‘i Æ°u cho cáº£ hai

### 3. Target Filtering
- âœ… Chá»‰ detect digits: `--target digits`
- âœ… Chá»‰ detect shapes: `--target shapes`
- âœ… Detect cáº£ hai: `--target all`

### 4. Synthetic Data Generation
- âœ… Tá»± Ä‘á»™ng táº¡o test images
- âœ… Control sá»‘ lÆ°á»£ng digits vÃ  shapes
- âœ… KhÃ´ng overlap giá»¯a cÃ¡c objects
- âœ… Ground truth labels

### 5. MQTT Real-time Processing
- âœ… Nháº­n áº£nh tá»« frontend qua MQTT
- âœ… Xá»­ lÃ½ vÃ  tráº£ káº¿t quáº£ real-time
- âœ… Base64 encoding cho images
- âœ… JSON format cho results

### 6. Reading Order Sorting
- âœ… Sort detections theo thá»© tá»± Ä‘á»c tá»± nhiÃªn
- âœ… Top-to-bottom, left-to-right
- âœ… Group objects vÃ o rows

## ğŸ“ˆ Káº¿t quáº£ vÃ  Performance

### Classification Accuracy

| Category | Training | Validation | Notes |
|----------|----------|------------|-------|
| **Overall** | 99.3% | **99.14%** | 19 classes combined |
| **Digits (0-9)** | 99.5% | 99.3% | High accuracy |
| **Shapes** | 99.0% | 98.5% | Good, some confusion Circle/Nonagon |
| **Best Class** | - | 99.90% | Digit "1", Triangle |
| **Worst Class** | - | 94.69% | Nonagon (confused with Circle) |

### Per-Class Performance

**Top Performers:**
- Digit "1": 99.90%
- Digit "8": 99.89%
- Triangle: 99.90%
- Star: 99.70%

**Challenging Classes:**
- Nonagon: 94.69% (confused with Circle ~4%)
- Octagon: 97.96% (confused with Circle ~0.78%)

### Inference Speed

| Component | Time (ms) | Notes |
|-----------|-----------|-------|
| **Detection (Traditional)** | 50-100 | Fast, CPU-friendly |
| **Detection (CRAFT)** | 100-200 | Slower, requires GPU |
| **Detection (Hybrid)** | 150-250 | Combines both |
| **Classification (per object)** | 5-10 | EfficientNet-B0 |
| **Total (5 objects)** | 100-300 | End-to-end |

*Tested on RTX 4050 Laptop GPU*

### Model Size

- **EfficientNet-B0**: ~5.3M parameters
- **Model weights**: ~20MB (.pth file)
- **CRAFT weights**: ~85MB
- **Total**: ~105MB

## ğŸ”„ Flow hoáº¡t Ä‘á»™ng chi tiáº¿t

### 1. Training Flow

```
Load Datasets (MNIST + Shapes)
    â†“
Create Label Mapping (0-18)
    â†“
Split Train/Val (85/15)
    â†“
Apply Augmentation
    â†“
Train EfficientNet-B0
    â†“
Validate & Save Best Model
    â†“
Evaluate Performance
```

### 2. Inference Flow

```
Input Image
    â†“
Preprocessing (Denoise, CLAHE, etc.)
    â†“
Detection (Traditional/CRAFT/Hybrid)
    â†“
Crop Bounding Boxes
    â†“
Resize to 128x128
    â†“
Classification (EfficientNet-B0)
    â†“
Filter by Target Classes
    â†“
Sort by Reading Order
    â†“
Visualize & Output JSON
```

### 3. MQTT Flow

```
Frontend â†’ image/create (numberDigit, numberShape)
    â†“
AI: Generate Synthetic Image
    â†“
AI â†’ image/input/create (image base64 + count)
    â†“
Frontend: Display Image
    â†“
User: Click "Process"
    â†“
Frontend â†’ image/input (image base64 + label + count)
    â†“
AI: Detect & Classify (Auto Hybrid if count exists)
    â†“
AI â†’ image/output (image base64 + detections JSON)
    â†“
Frontend: Display Results
```

## ğŸ“– MÃ´ táº£ chi tiáº¿t

Dá»± Ã¡n nÃ y xÃ¢y dá»±ng má»™t há»‡ thá»‘ng hoÃ n chá»‰nh Ä‘á»ƒ nháº­n diá»‡n vÃ  phÃ¢n loáº¡i:
- **10 chá»¯ sá»‘**: 0-9 (tá»« MNIST dataset)
- **9 hÃ¬nh há»c**: Circle, Triangle, Square, Pentagon, Hexagon, Heptagon, Octagon, Nonagon, Star

### Pipeline

```
Input Image â†’ Detection (Traditional CV/CRAFT) â†’ Classification (EfficientNet-B0) â†’ Output (Annotated Image + JSON)
```

### Äáº·c Ä‘iá»ƒm ná»•i báº­t

- âœ… **19 classes**: Digits (0-9) + Shapes (9 loáº¡i)
- âœ… **Äá»™ chÃ­nh xÃ¡c cao**: ~99% validation accuracy
- âœ… **Inference nhanh**: ~100-300ms/áº£nh
- âœ… **Linh hoáº¡t**: Há»— trá»£ nhiá»u phÆ°Æ¡ng phÃ¡p detection
- âœ… **Dá»… sá»­ dá»¥ng**: API Ä‘Æ¡n giáº£n vÃ  rÃµ rÃ ng
- âœ… **MQTT Integration**: Real-time processing vá»›i frontend
- âœ… **Synthetic Data Generation**: Tá»± Ä‘á»™ng táº¡o test images

## ğŸš€ CÃ i Ä‘áº·t

### YÃªu cáº§u há»‡ thá»‘ng

- Python 3.8+
- CUDA 11.8+ (optional, cho GPU acceleration)
- RAM: 8GB+ (16GB recommended)
- Disk space: ~5GB (cho datasets vÃ  models)

### BÆ°á»›c 1: Clone repository

```bash
# Clone project tá»« GitHub
git clone https://github.com/your-username/BTL_XLA.git
cd BTL_XLA
```

### BÆ°á»›c 2: Setup Conda Environment (Khuyáº¿n nghá»‹)

```bash
# Táº¡o conda environment má»›i
conda create -n btl_xla python=3.10 -y

# Activate environment
conda activate btl_xla

# CÃ i Ä‘áº·t PyTorch vá»›i CUDA (náº¿u cÃ³ GPU)
conda install pytorch torchvision pytorch-cuda=11.8 -c pytorch -c nvidia -y

# CÃ i Ä‘áº·t cÃ¡c dependencies cÃ²n láº¡i
pip install -r requirements.txt
```

**Hoáº·c náº¿u chá»‰ dÃ¹ng CPU:**

```bash
conda create -n btl_xla python=3.10 -y
conda activate btl_xla
conda install pytorch torchvision cpuonly -c pytorch -y
pip install -r requirements.txt
```

### BÆ°á»›c 3: Táº£i vÃ  chuáº©n bá»‹ dá»¯ liá»‡u

#### 3.1. MNIST Dataset

```bash
# Giáº£i nÃ©n mnist_competition.zip (náº¿u cÃ³)
unzip mnist_competition.zip

# Hoáº·c táº£i tá»« Kaggle/Google Drive
# Cáº¥u trÃºc: mnist_competition/train/ vÃ  mnist_competition/train_label.csv
```

#### 3.2. Shapes Dataset

```bash
# Giáº£i nÃ©n dataset trong Shapes_Classifier
cd Shapes_Classifier
unzip dataset.zip
cd ..
```

#### 3.3. CRAFT Weights (cho Hybrid Detector)

```bash
# Táº¡o thÆ° má»¥c weights
mkdir weights

# Táº£i CRAFT weights
wget https://drive.google.com/uc?id=1Jk4eGD7crsqCCg9C9VjCLkMN3ze8kutZ -O weights/craft_mlt_25k.pth

# Hoáº·c dÃ¹ng gdown
pip install gdown
gdown https://drive.google.com/uc?id=1Jk4eGD7crsqCCg9C9VjCLkMN3ze8kutZ -O weights/craft_mlt_25k.pth
```

### Chuáº©n bá»‹ dá»¯ liá»‡u hoÃ n táº¥t

Äáº£m báº£o cáº¥u trÃºc thÆ° má»¥c nhÆ° sau:

```
BTL_XLA/
â”œâ”€â”€ mnist_competition/
â”‚   â”œâ”€â”€ train/              # 60,000 MNIST images
â”‚   â”œâ”€â”€ train_label.csv
â”‚   â””â”€â”€ public_test/
â”œâ”€â”€ Shapes_Classifier/
â”‚   â””â”€â”€ dataset/output/     # 90,000 shape images (Circle, Square, etc.)
â”œâ”€â”€ weights/
â”‚   â””â”€â”€ craft_mlt_25k.pth   # CRAFT pretrained weights (~85MB)
â”œâ”€â”€ unified_model_19classes_best.pth  # Trained classifier
â””â”€â”€ label_mapping.json
```

## ğŸ“š HÆ°á»›ng dáº«n sá»­ dá»¥ng

### 1. Training Classifier

Train mÃ´ hÃ¬nh EfficientNet-B0 trÃªn 19 classes:

#### Sá»­ dá»¥ng Python Script

```bash
python train_unified_classifier.py --epochs 20 --batch-size 64
```

**Tham sá»‘:**
- `--epochs`: Sá»‘ epoch (máº·c Ä‘á»‹nh: 20)
- `--batch-size`: Batch size (máº·c Ä‘á»‹nh: 64)
- `--lr`: Learning rate (máº·c Ä‘á»‹nh: 1e-4)
- `--device`: 'cuda' hoáº·c 'cpu'

#### Sá»­ dá»¥ng Jupyter Notebook

```bash
jupyter notebook train_unified_classifier.ipynb
```

**Output:**
- `unified_model_19classes_best.pth`: Model Ä‘Ã£ train
- `label_mapping.json`: Ãnh xáº¡ class labels
- `training_history.png`: Biá»ƒu Ä‘á»“ loss/accuracy

### 2. Pipeline - Inference trÃªn áº£nh

#### 2.1. Xá»­ lÃ½ áº£nh cÃ³ sáºµn (táº¥t cáº£ classes)

```bash
python pipeline.py --image Sample.png --output Sample_result.png
```

#### 2.2. Chá»‰ nháº­n diá»‡n SHAPES

```bash
python pipeline.py --image Sample.png --target shapes --output Sample_shapes_only.png
```

#### 2.3. Chá»‰ nháº­n diá»‡n DIGITS

```bash
python pipeline.py --image Sample.png --target digits --output Sample_digits_only.png
```

#### 2.4. Sá»­ dá»¥ng Hybrid Detector (CRAFT + Traditional CV)

```bash
python pipeline.py --image Sample.png --detector hybrid --target all
```

#### 2.5. Táº¡o áº£nh test synthetic tá»± Ä‘á»™ng

```bash
# Táº¡o áº£nh vá»›i 5 objects (máº·c Ä‘á»‹nh)
python pipeline.py --generate

# Táº¡o áº£nh vá»›i 10 objects
python pipeline.py --generate --num-objects 10

# Táº¡o vÃ  chá»‰ detect shapes
python pipeline.py --generate --num-objects 8 --target shapes
```

**Pipeline Output:**
- `*_result.png`: áº¢nh Ä‘Æ°á»£c annotate vá»›i bounding boxes
- `*_result.json`: Káº¿t quáº£ detection á»Ÿ Ä‘á»‹nh dáº¡ng JSON

**Pipeline Arguments:**

| Argument | Choices | Default | MÃ´ táº£ |
|----------|---------|---------|-------|
| `--image` | path | None | ÄÆ°á»ng dáº«n áº£nh input |
| `--output` | path | Auto | ÄÆ°á»ng dáº«n áº£nh output |
| `--target` | `digits`, `shapes`, `all` | `all` | Loáº¡i objects cáº§n detect |
| `--detector` | `traditional`, `hybrid` | `traditional` | PhÆ°Æ¡ng phÃ¡p detection |
| `--generate` | flag | False | Táº¡o áº£nh test synthetic |
| `--num-objects` | int | 5 | Sá»‘ objects trong synthetic scene |
| `--model` | path | `unified_model_19classes_best.pth` | Model weights |
| `--labels` | path | `label_mapping.json` | Label mapping |
| `--device` | `cuda`, `cpu` | Auto | Device Ä‘á»ƒ inference |

### 3. Evaluation

ÄÃ¡nh giÃ¡ hiá»‡u nÄƒng model:

```bash
python evaluate_model.py
```

**Output:**
- Per-class accuracy report
- Confusion matrix
- Classification report
- `per_class_performance.csv`

### 4. Sá»­ dá»¥ng nhÆ° má»™t module

```python
from pipeline import UnifiedPipeline

# Khá»Ÿi táº¡o pipeline - Detect ALL
pipeline = UnifiedPipeline(
    model_path='unified_model_19classes_best.pth',
    label_mapping_path='label_mapping.json',
    device='cuda',  # hoáº·c 'cpu'
    detector_type='traditional',  # hoáº·c 'hybrid'
    target_classes='all'  # 'digits', 'shapes', hoáº·c 'all'
)

# Xá»­ lÃ½ áº£nh
results = pipeline.process_file('test_image.png')

# Káº¿t quáº£
print(f"Detected {len(results['labels'])} objects")
for label, conf in zip(results['labels'], results['confidences']):
    print(f"Class: {label}, Confidence: {conf:.2%}")
```

#### Táº¡o synthetic data

```python
from pipeline import generate_synthetic_scene
import cv2

# Táº¡o scene vá»›i 10 random objects
canvas, ground_truth = generate_synthetic_scene(
    mnist_dir='mnist_competition/train',
    shapes_dir='Shapes_Classifier/dataset/output',
    mnist_csv='mnist_competition/train_label.csv',
    num_objects=10,
    canvas_size=(800, 600),
    seed=42
)

# LÆ°u áº£nh
cv2.imwrite('my_test_scene.png', canvas)

# In ground truth
print("Ground truth labels:", [item[4] for item in ground_truth])
```

## ğŸ“ Cáº¥u trÃºc dá»± Ã¡n

```
BTL_XLA/
â”œâ”€â”€ mnist_competition/              # MNIST dataset
â”‚   â”œâ”€â”€ train/                      # Training images
â”‚   â”œâ”€â”€ public_test/                # Test images
â”‚   â””â”€â”€ train_label.csv             # Labels
â”œâ”€â”€ Shapes_Classifier/              # Shapes dataset
â”‚   â””â”€â”€ dataset/output/             # Shape images
â”œâ”€â”€ train_unified_classifier.py     # Training script
â”œâ”€â”€ detect_objects.py               # Detection module
â”œâ”€â”€ pipeline.py                     # End-to-end pipeline
â”œâ”€â”€ preprocess_grid_image.py        # Preprocessing utilities
â”œâ”€â”€ unified_model_19classes_best.pth    # Trained model
â”œâ”€â”€ label_mapping.json              # Class mapping
â”œâ”€â”€ requirements.txt                # Dependencies
â”œâ”€â”€ CRAFT_SHAPES_GUIDE.md          # Detailed guide
â””â”€â”€ README.md                       # This file
```

## ğŸ¯ Class Mapping

| Class ID | Label | Category |
|----------|-------|----------|
| 0-9 | 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 | Digits |
| 10 | Circle | Shape |
| 11 | Heptagon | Shape |
| 12 | Hexagon | Shape |
| 13 | Nonagon | Shape |
| 14 | Octagon | Shape |
| 15 | Pentagon | Shape |
| 16 | Square | Shape |
| 17 | Star | Shape |
| 18 | Triangle | Shape |

## ğŸ“Š Hiá»‡u nÄƒng

### Classification Accuracy

| Dataset | Training | Validation |
|---------|----------|------------|
| MNIST Digits | 99.5% | 99.3% |
| Shapes | 99.0% | 98.5% |
| **Unified (19 classes)** | **99.3%** | **99.0%** |

### Inference Speed

| Component | Time (ms) |
|-----------|-----------|
| Detection | 50-150 |
| Classification | 5-10 per object |
| **Total** | **100-300** |

*Tested on RTX 4050*

## ğŸ”§ Advanced Usage

### Custom Detection Parameters

```python
from detect_objects import TraditionalDetector

detector = TraditionalDetector(
    min_area=200,
    max_area=30000,
    aspect_ratio_range=(0.2, 5.0)
)

bboxes = detector.detect(image)
```

### Synthetic Data Generation

```python
from pipeline import generate_synthetic_scene

canvas, ground_truth = generate_synthetic_scene(
    mnist_dir='mnist_competition/train',
    shapes_dir='Shapes_Classifier/dataset/output',
    mnist_csv='mnist_competition/train_label.csv',
    num_objects=5,
    canvas_size=(800, 600),
    seed=42
)
```

## ğŸ”„ HÆ°á»›ng dáº«n Push/Pull vá»›i GitHub (Sá»­ dá»¥ng Conda)

### Láº§n Ä‘áº§u push lÃªn GitHub

#### BÆ°á»›c 1: Táº¡o repository trÃªn GitHub

1. VÃ o [GitHub](https://github.com)
2. Click **New repository**
3. Äáº·t tÃªn: `BTL_XLA`
4. Chá»n **Public** hoáº·c **Private**
5. **KHÃ”NG** chá»n "Initialize with README"
6. Click **Create repository**

#### BÆ°á»›c 2: Setup Git local (náº¿u chÆ°a cÃ³)

```bash
# Kiá»ƒm tra Git Ä‘Ã£ cÃ i chÆ°a
git --version

# Náº¿u chÆ°a cÃ³, cÃ i Git
# Windows: Download tá»« https://git-scm.com/
# Linux: sudo apt install git
# macOS: brew install git

# Config thÃ´ng tin (chá»‰ cáº§n 1 láº§n)
git config --global user.name "Your Name"
git config --global user.email "your.email@example.com"
```

#### BÆ°á»›c 3: Khá»Ÿi táº¡o Git repository

```bash
# Activate conda environment
conda activate btl_xla

# Di chuyá»ƒn vÃ o thÆ° má»¥c project
cd D:\BTL_XLA

# Khá»Ÿi táº¡o Git repository
git init

# Kiá»ƒm tra status
git status
```

#### BÆ°á»›c 4: Add files vÃ  commit

```bash
# Add táº¥t cáº£ files (theo .gitignore)
git add .

# Kiá»ƒm tra nhá»¯ng gÃ¬ sáº½ commit
git status

# Commit láº§n Ä‘áº§u
git commit -m "Initial commit: Unified Digits & Shapes Recognition System"
```

#### BÆ°á»›c 5: Káº¿t ná»‘i vá»›i GitHub vÃ  push

```bash
# ThÃªm remote repository (thay YOUR_USERNAME báº±ng username GitHub cá»§a báº¡n)
git remote add origin https://github.com/YOUR_USERNAME/BTL_XLA.git

# Kiá»ƒm tra remote
git remote -v

# Push lÃªn GitHub (branch main)
git branch -M main
git push -u origin main
```

**LÆ°u Ã½ vá» viá»‡c push:**
- Theo `.gitignore`, nhá»¯ng thá»© SAU sáº½ Ä‘Æ°á»£c push:
  - âœ… `craft_repo/` (full folder)
  - âœ… `mnist_competition.zip` (file nÃ©n)
  - âœ… `mnist_competition/*.csv` (cÃ¡c file CSV)
  - âœ… `Shapes_Classifier/` (trá»« folder `dataset/`)
  - âœ… `weights/craft_mlt_25k.pth`
  - âœ… `unified_model_19classes_best.pth`
  - âœ… Táº¥t cáº£ `.py`, `.ipynb`, `.md`, `requirements.txt`
  - âœ… `Sample.png`, `label_mapping.json`

- Nhá»¯ng thá»© SAU sáº½ KHÃ”NG push (Ä‘Ã£ bá»‹ ignore):
  - âŒ `mnist_competition/train/` (60,000 áº£nh)
  - âŒ `mnist_competition/public_test/` (10,000 áº£nh)
  - âŒ `Shapes_Classifier/dataset/` (90,000 áº£nh)
  - âŒ `__pycache__/`, `.ipynb_checkpoints/`
  - âŒ `*_result.png`, `*_result.json`
  - âŒ `Test_*.png`, `Test_*.jpg`

### Khi muá»‘n update code (push thay Ä‘á»•i má»›i)

```bash
# Activate environment
conda activate btl_xla

# Kiá»ƒm tra thay Ä‘á»•i
git status

# Add files Ä‘Ã£ thay Ä‘á»•i
git add .

# Commit vá»›i message mÃ´ táº£
git commit -m "Update: Improved detection accuracy"

# Push lÃªn GitHub
git push origin main
```

### Khi muá»‘n táº£i code má»›i (pull tá»« GitHub)

```bash
# Pull code má»›i nháº¥t
git pull origin main

# Náº¿u bá»‹ conflict, Git sáº½ bÃ¡o - cáº§n resolve manually
```

### Clone project tá»« GitHub (cho mÃ¡y khÃ¡c)

```bash
# Clone repository
git clone https://github.com/YOUR_USERNAME/BTL_XLA.git
cd BTL_XLA

# Setup conda environment
conda create -n btl_xla python=3.10 -y
conda activate btl_xla

# CÃ i Ä‘áº·t dependencies
conda install pytorch torchvision pytorch-cuda=11.8 -c pytorch -c nvidia -y
pip install -r requirements.txt

# Giáº£i nÃ©n datasets
unzip mnist_competition.zip
cd Shapes_Classifier
unzip dataset.zip
cd ..

# Cháº¡y pipeline
python pipeline.py --generate --num-objects 5
```

### Git Commands thÆ°á»ng dÃ¹ng

```bash
# Xem lá»‹ch sá»­ commit
git log --oneline

# Xem thay Ä‘á»•i chÆ°a commit
git diff

# Há»§y thay Ä‘á»•i chÆ°a add
git restore filename.py

# Táº¡o branch má»›i
git checkout -b feature/new-feature

# Chuyá»ƒn branch
git checkout main

# Merge branch
git merge feature/new-feature

# Xem táº¥t cáº£ branches
git branch -a
```

## ğŸ› Troubleshooting

### Lá»—i: Model khÃ´ng load Ä‘Æ°á»£c

```bash
# Kiá»ƒm tra PyTorch version
python -c "import torch; print(torch.__version__)"

# Kiá»ƒm tra CUDA
python -c "import torch; print(torch.cuda.is_available())"
```

### Lá»—i: Out of memory

- Giáº£m batch size: `--batch-size 32`
- Sá»­ dá»¥ng CPU: `--device cpu`
- Giáº£m resolution cá»§a áº£nh input

### Detection rate tháº¥p

- Äiá»u chá»‰nh threshold: `min_area=100`
- Thá»­ detector khÃ¡c: `--detector hybrid`

## ğŸ“„ TÃ i liá»‡u tham kháº£o

- [CRAFT Paper](https://arxiv.org/abs/1904.01941) - Character Region Awareness For Text detection
- [EfficientNet Paper](https://arxiv.org/abs/1905.11946) - Efficient Convolutional Neural Networks
- [CRAFT GitHub](https://github.com/clovaai/CRAFT-pytorch) - Official CRAFT implementation
- [Detailed Guide](CRAFT_SHAPES_GUIDE.md) - HÆ°á»›ng dáº«n chi tiáº¿t

## ğŸ¤ ÄÃ³ng gÃ³p

Má»i Ä‘Ã³ng gÃ³p Ä‘á»u Ä‘Æ°á»£c hoan nghÃªnh! Vui lÃ²ng:

1. Fork repository
2. Táº¡o feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Má»Ÿ Pull Request

## ğŸ“ License

Dá»± Ã¡n nÃ y Ä‘Æ°á»£c phÃ¡t triá»ƒn cho má»¥c Ä‘Ã­ch giÃ¡o dá»¥c.

## ğŸ‘¥ TÃ¡c giáº£

- Äá»“ Ã¡n mÃ´n Xá»­ lÃ½ áº£nh (Image Processing)
- TrÆ°á»ng Äáº¡i há»c...

## ğŸ™ Acknowledgments

- MNIST Dataset
- Shapes Dataset
- CRAFT-pytorch
- EfficientNet

---

**â­ Náº¿u project há»¯u Ã­ch, Ä‘á»«ng quÃªn cho má»™t star nhÃ©!**

## ğŸ“ LiÃªn há»‡

Náº¿u cÃ³ cÃ¢u há»i hoáº·c váº¥n Ä‘á», vui lÃ²ng táº¡o [Issue](../../issues) trÃªn GitHub.

